"""
ç¥ç»å…ƒä¸»è¦åˆ†æå™¨ - EMtrace01 æ•°æ®åˆ†æè„šæœ¬

è¯¥è„šæœ¬ç”¨äºåˆ†æç¥ç»å…ƒæ´»åŠ¨æ•°æ®ï¼ŒåŒ…æ‹¬æ•ˆåº”é‡è®¡ç®—ã€å…³é”®ç¥ç»å…ƒè¯†åˆ«å’Œå¯è§†åŒ–ã€‚
æ‰€æœ‰çš„è·¯å¾„é…ç½®éƒ½ç»Ÿä¸€ç®¡ç†åœ¨æ–‡ä»¶å¼€å¤´çš„PathConfigç±»ä¸­ï¼Œæ–¹ä¾¿ä¿®æ”¹å’Œç»´æŠ¤ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
1. ä¿®æ”¹PathConfigç±»ä¸­çš„è·¯å¾„å˜é‡æ¥æŒ‡å®šè¾“å…¥è¾“å‡ºæ–‡ä»¶
2. åœ¨mainå‡½æ•°ä¸­ä¿®æ”¹dataset_keyæ¥åˆ‡æ¢ä¸åŒçš„æ•°æ®é›†
3. è¿è¡Œè„šæœ¬å³å¯ç”Ÿæˆåˆ†æç»“æœå’Œå¯è§†åŒ–å›¾è¡¨

ä½œè€…: Assistant
æ—¥æœŸ: 2025å¹´
"""

import pandas as pd
import numpy as np
import os
from itertools import combinations # Add this import for combinations

# ===============================================================================
# è·¯å¾„é…ç½®éƒ¨åˆ† - æ‰€æœ‰è¾“å…¥è¾“å‡ºè·¯å¾„çš„ç»Ÿä¸€ç®¡ç†
# ===============================================================================

class PathConfig:
    """
    è·¯å¾„é…ç½®ç±»ï¼šé›†ä¸­ç®¡ç†æ‰€æœ‰è¾“å…¥è¾“å‡ºè·¯å¾„é…ç½®
    
    åœ¨è¿™é‡Œç»Ÿä¸€ä¿®æ”¹æ‰€æœ‰æ–‡ä»¶è·¯å¾„ï¼Œä¾¿äºç®¡ç†å’Œç»´æŠ¤
    
    ä½¿ç”¨æ–¹æ³•ï¼š
    --------
    1. ä¿®æ”¹ä»¥ä¸‹è·¯å¾„å˜é‡æ¥æ”¹å˜è¾“å…¥è¾“å‡ºç›®å½•
    2. ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨äºæŒ‡å®šè·¯å¾„
    3. ç¨‹åºå°†è‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•
    """
    
    def __init__(self):
        # === è¾“å‡ºç›®å½•é…ç½® ===
        # è·å–å½“å‰è„šæœ¬çš„ç›®å½•ï¼Œç„¶åæ„å»ºç»å¯¹è·¯å¾„
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_dir = os.path.dirname(current_dir)  # ä¸Šä¸€çº§ç›®å½•ï¼ˆprincipal_neuronï¼‰
        
        self.BASE_OUTPUT_DIR = os.path.join(project_dir, "output_plots")  # åŸºç¡€è¾“å‡ºç›®å½•
        self.BASE_EFFECT_SIZE_OUTPUT_DIR = os.path.join(project_dir, "effect_size_output")  # åŸºç¡€æ•ˆåº”é‡è¾“å‡ºç›®å½•
        self.DATA_DIR = os.path.join(project_dir, "data")  # æ•°æ®ç›®å½•
        
        # === æ•°æ®é›†é…ç½®ï¼šå®Œæ•´çš„æ•°æ®é›†æ¸…å• ===
        # æ¯ä¸ªæ•°æ®é›†åŒ…å«ä¸‰ä¸ªæ–‡ä»¶ï¼šåŸå§‹æ•°æ®ã€æ•ˆåº”é‡æ•°æ®ã€ä½ç½®æ•°æ®
        self.DATASETS = {
            # EMtraceç³»åˆ—æ•°æ®é›†
            'emtrace01': {
                'name': 'EMtrace01æ•°æ®é›†',
                'raw': os.path.join(self.DATA_DIR, 'EMtrace01.xlsx'),
                'effect': os.path.join(self.DATA_DIR, 'EMtrace01-3æ ‡ç­¾ç‰ˆ.csv'),
                'position': os.path.join(self.DATA_DIR, 'EMtrace01_Max_position.csv'),
                'description': 'EMtrace01ç¥ç»å…ƒæ´»åŠ¨æ•°æ®ï¼ˆ3æ ‡ç­¾ç‰ˆï¼‰'
            },
            'emtrace01_plus': {
                'name': 'EMtrace01å¢å¼ºæ•°æ®é›†',
                'raw': os.path.join(self.DATA_DIR, 'EMtrace01_plus.xlsx'),
                'effect': os.path.join(self.DATA_DIR, 'EMtrace01-3æ ‡ç­¾ç‰ˆ.csv'),  # å¤ç”¨åŒä¸€ä¸ªæ•ˆåº”é‡æ–‡ä»¶
                'position': os.path.join(self.DATA_DIR, 'EMtrace01_Max_position.csv'),
                'description': 'EMtrace01å¢å¼ºç‰ˆç¥ç»å…ƒæ´»åŠ¨æ•°æ®'
            },
            'emtrace02': {
                'name': 'EMtrace02æ•°æ®é›†',
                'raw': os.path.join(self.DATA_DIR, 'EMtrace02.xlsx'),
                'effect': os.path.join(self.DATA_DIR, 'EMtrace02-3æ ‡ç­¾ç‰ˆ.csv'),
                'position': os.path.join(self.DATA_DIR, 'EMtrace02_Max_position.csv'),
                'description': 'EMtrace02ç¥ç»å…ƒæ´»åŠ¨æ•°æ®ï¼ˆ3æ ‡ç­¾ç‰ˆï¼‰'
            },
            'emtrace02_plus': {
                'name': 'EMtrace02å¢å¼ºæ•°æ®é›†',
                'raw': os.path.join(self.DATA_DIR, 'EMtrace02_plus.xlsx'),
                'effect': os.path.join(self.DATA_DIR, 'EMtrace02-3æ ‡ç­¾ç‰ˆ.csv'),
                'position': os.path.join(self.DATA_DIR, 'EMtrace02_Max_position.csv'),
                'description': 'EMtrace02å¢å¼ºç‰ˆç¥ç»å…ƒæ´»åŠ¨æ•°æ®'
            },
            
            # å…¶ä»–æ•°æ®é›†
            '2980': {
                'name': '2980 datasets',
                'raw': os.path.join(self.DATA_DIR, '2980240924EMtrace.xlsx'),
                'effect': os.path.join(self.BASE_EFFECT_SIZE_OUTPUT_DIR, 'effect_sizes_2980240924EMtrace.csv'),
                'position': os.path.join(self.DATA_DIR, '2980_Max_position.csv'),
                'description': '2980ç¥ç»å…ƒæ´»åŠ¨æ•°æ®'
            },
            '2980_plus': {
                'name': '2980 datasets',
                'raw': os.path.join(self.DATA_DIR, '2980240924EMtrace_plus.xlsx'),
                'effect': os.path.join(self.BASE_EFFECT_SIZE_OUTPUT_DIR, 'effect_sizes_2980240924EMtrace_plus.csv'),
                'position': os.path.join(self.DATA_DIR, '2980_Max_position.csv'),
                'description': '2980å¢å¼ºç‰ˆç¥ç»å…ƒæ´»åŠ¨æ•°æ®'
            },
            'bla6250': {
                'name': 'BLA6250 datasets',
                'raw': os.path.join(self.DATA_DIR, 'bla6250EM0626goodtrace.xlsx'),
                'effect': os.path.join(self.BASE_EFFECT_SIZE_OUTPUT_DIR, 'effect_sizes_bla6250EM0626goodtrace.csv'),
                'position': os.path.join(self.DATA_DIR, '6250_Max_position.csv'),
                'description': 'BLA6250ç¥ç»å…ƒæ´»åŠ¨æ•°æ®'
            },
            'bla6250_plus': {
                'name': 'BLA6250 datasets',
                'raw': os.path.join(self.DATA_DIR, 'bla6250EM0626goodtrace_plus.xlsx'),
                'effect': os.path.join(self.BASE_EFFECT_SIZE_OUTPUT_DIR, 'effect_sizes_bla6250EM0626goodtrace_plus.csv'),
                'position': os.path.join(self.DATA_DIR, '6250_Max_position.csv'),
                'description': 'BLA6250å¢å¼ºç‰ˆç¥ç»å…ƒæ´»åŠ¨æ•°æ®'
            },
            
            # Dayç³»åˆ—æ•°æ®é›†
            'day3': {
                'name': 'Day3æ•°æ®é›†',
                'raw': None,  # åªæœ‰æ•ˆåº”é‡æ•°æ®
                'effect': os.path.join(self.DATA_DIR, 'day3.csv'),
                'position': os.path.join(self.DATA_DIR, 'Day3_Max_position.csv'),
                'description': 'Day3ç¥ç»å…ƒæ´»åŠ¨æ•°æ®'
            },
            'day6': {
                'name': 'Day6æ•°æ®é›†',
                'raw': None,
                'effect': os.path.join(self.DATA_DIR, 'day6.csv'),
                'position': os.path.join(self.DATA_DIR, 'Day6_Max_position.csv'),
                'description': 'Day6ç¥ç»å…ƒæ´»åŠ¨æ•°æ®'
            },
            'day9': {
                'name': 'Day9æ•°æ®é›†',
                'raw': None,
                'effect': os.path.join(self.DATA_DIR, 'day9.csv'),
                'position': os.path.join(self.DATA_DIR, 'Day9_Max_position.csv'),
                'description': 'Day9ç¥ç»å…ƒæ´»åŠ¨æ•°æ®'
            }
        }
        
        # === é»˜è®¤æ•°æ®é›†è®¾ç½® ===
        self.DEFAULT_DATASET = 'emtrace01'  # é»˜è®¤ä½¿ç”¨EMtrace01æ•°æ®é›†
        
        # === åˆ›å»ºå¿…è¦çš„ç›®å½• ===
        self._ensure_base_directories()
    
    def _ensure_base_directories(self):
        """ç¡®ä¿åŸºç¡€è¾“å‡ºç›®å½•å­˜åœ¨"""
        base_directories = [self.BASE_OUTPUT_DIR, self.BASE_EFFECT_SIZE_OUTPUT_DIR]
        for directory in base_directories:
            if not os.path.exists(directory):
                os.makedirs(directory)
                print(f"åˆ›å»ºåŸºç¡€è¾“å‡ºç›®å½•: {directory}")
    
    def get_dataset_output_dir(self, dataset_key):
        """
        è·å–æŒ‡å®šæ•°æ®é›†çš„ä¸“ç”¨è¾“å‡ºç›®å½•
        
        å‚æ•°:
            dataset_key: æ•°æ®é›†é”®å
            
        è¿”å›:
            str: æ•°æ®é›†ä¸“ç”¨è¾“å‡ºç›®å½•è·¯å¾„
        """
        if dataset_key not in self.DATASETS:
            raise ValueError(f"æœªçŸ¥çš„æ•°æ®é›†é”®å: {dataset_key}")
        
        dataset_output_dir = os.path.join(self.BASE_OUTPUT_DIR, dataset_key)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(dataset_output_dir):
            os.makedirs(dataset_output_dir)
            print(f"åˆ›å»ºæ•°æ®é›†ä¸“ç”¨è¾“å‡ºç›®å½•: {dataset_output_dir}")
            
        return dataset_output_dir
    
    def get_dataset_effect_size_output_dir(self, dataset_key):
        """
        è·å–æŒ‡å®šæ•°æ®é›†çš„ä¸“ç”¨æ•ˆåº”é‡è¾“å‡ºç›®å½•
        
        å‚æ•°:
            dataset_key: æ•°æ®é›†é”®å
            
        è¿”å›:
            str: æ•°æ®é›†ä¸“ç”¨æ•ˆåº”é‡è¾“å‡ºç›®å½•è·¯å¾„
        """
        if dataset_key not in self.DATASETS:
            raise ValueError(f"æœªçŸ¥çš„æ•°æ®é›†é”®å: {dataset_key}")
        
        effect_size_output_dir = os.path.join(self.BASE_EFFECT_SIZE_OUTPUT_DIR, dataset_key)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(effect_size_output_dir):
            os.makedirs(effect_size_output_dir)
            print(f"åˆ›å»ºæ•°æ®é›†ä¸“ç”¨æ•ˆåº”é‡è¾“å‡ºç›®å½•: {effect_size_output_dir}")
            
        return effect_size_output_dir
    
    def get_data_paths(self, dataset_key=None):
        """
        è·å–æŒ‡å®šæ•°æ®é›†çš„æ‰€æœ‰è·¯å¾„
        
        å‚æ•°:
            dataset_key: æ•°æ®é›†é”®åï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ•°æ®é›†
        
        è¿”å›:
            dict: åŒ…å«raw, effect, positionä¸‰ä¸ªè·¯å¾„çš„å­—å…¸
        """
        if dataset_key is None:
            dataset_key = self.DEFAULT_DATASET
            
        if dataset_key not in self.DATASETS:
            raise ValueError(f"æœªçŸ¥çš„æ•°æ®é›†é”®å: {dataset_key}ã€‚å¯ç”¨æ•°æ®é›†: {list(self.DATASETS.keys())}")
        
        dataset_info = self.DATASETS[dataset_key]
        return {
            'raw': dataset_info['raw'],
            'effect': dataset_info['effect'],
            'position': dataset_info['position'],
            'name': dataset_info['name'],
            'description': dataset_info['description'],
            'output_dir': self.get_dataset_output_dir(dataset_key),  # æ·»åŠ ä¸“ç”¨è¾“å‡ºç›®å½•
            'effect_size_output_dir': self.get_dataset_effect_size_output_dir(dataset_key)  # æ·»åŠ ä¸“ç”¨æ•ˆåº”é‡è¾“å‡ºç›®å½•
        }
    
    def list_available_datasets(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†"""
        print("=" * 60)
        print("å¯ç”¨çš„æ•°æ®é›†:")
        print("=" * 60)
        for key, dataset in self.DATASETS.items():
            print(f"\nğŸ“ æ•°æ®é›†é”®å: '{key}'")
            print(f"   åç§°: {dataset['name']}")
            print(f"   æè¿°: {dataset['description']}")
            print(f"   åŸå§‹æ•°æ®: {dataset['raw'] or 'æ— '}")
            print(f"   æ•ˆåº”é‡æ•°æ®: {dataset['effect'] or 'éœ€è¦è®¡ç®—'}")
            print(f"   ä½ç½®æ•°æ®: {dataset['position'] or 'æ— '}")
            print(f"   è¾“å‡ºç›®å½•: {self.BASE_OUTPUT_DIR}/{key}/")
        print("=" * 60)
    
    def check_dataset_availability(self, dataset_key=None):
        """
        æ£€æŸ¥æŒ‡å®šæ•°æ®é›†çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        
        å‚æ•°:
            dataset_key: æ•°æ®é›†é”®å
            
        è¿”å›:
            dict: åŒ…å«å„æ–‡ä»¶å­˜åœ¨çŠ¶æ€çš„å­—å…¸
        """
        if dataset_key is None:
            dataset_key = self.DEFAULT_DATASET
            
        paths = self.get_data_paths(dataset_key)
        availability = {
            'dataset_key': dataset_key,
            'dataset_name': paths['name'],
            'raw_exists': paths['raw'] and os.path.exists(paths['raw']),
            'effect_exists': paths['effect'] and os.path.exists(paths['effect']),
            'position_exists': paths['position'] and os.path.exists(paths['position']),
            'raw_path': paths['raw'],
            'effect_path': paths['effect'],
            'position_path': paths['position'],
            'output_dir': paths['output_dir'],
            'effect_size_output_dir': paths['effect_size_output_dir']
        }
        
        availability['is_usable'] = (
            availability['position_exists'] and 
            (availability['effect_exists'] or availability['raw_exists'])
        )
        
        return availability
    
    def print_dataset_status(self, dataset_key=None):
        """æ‰“å°æ•°æ®é›†çš„è¯¦ç»†çŠ¶æ€ä¿¡æ¯"""
        status = self.check_dataset_availability(dataset_key)
        
        print(f"\nğŸ“Š æ•°æ®é›†çŠ¶æ€æ£€æŸ¥: {status['dataset_name']} ('{status['dataset_key']}')")
        print("-" * 50)
        
        # æ£€æŸ¥å„æ–‡ä»¶çŠ¶æ€
        files_to_check = [
            ('åŸå§‹æ•°æ®æ–‡ä»¶', status['raw_path'], status['raw_exists']),
            ('æ•ˆåº”é‡æ•°æ®æ–‡ä»¶', status['effect_path'], status['effect_exists']),
            ('ä½ç½®æ•°æ®æ–‡ä»¶', status['position_path'], status['position_exists'])
        ]
        
        for file_type, file_path, exists in files_to_check:
            if file_path:
                status_icon = "âœ…" if exists else "âŒ"
                print(f"{status_icon} {file_type}: {file_path}")
            else:
                print(f"âšª {file_type}: æ— ")
        
        # è¾“å‡ºç›®å½•ä¿¡æ¯
        print(f"\nğŸ“‚ è¾“å‡ºç›®å½•:")
        print(f"   å›¾è¡¨è¾“å‡º: {status['output_dir']}")
        print(f"   æ•ˆåº”é‡è¾“å‡º: {status['effect_size_output_dir']}")
        
        # æ€»ä½“å¯ç”¨æ€§
        if status['is_usable']:
            print(f"\nâœ… æ•°æ®é›†å¯ç”¨ï¼")
        else:
            print(f"\nâŒ æ•°æ®é›†ä¸å¯ç”¨ - ç¼ºå°‘å¿…è¦æ–‡ä»¶")
            
        return status
    
    def get_recommended_dataset(self):
        """è·å–æ¨èçš„å¯ç”¨æ•°æ®é›†"""
        # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥æ•°æ®é›†
        priority_order = ['emtrace01', 'emtrace02', 'emtrace01_plus', 'emtrace02_plus', '2980', 'bla6250']
        
        for dataset_key in priority_order:
            if dataset_key in self.DATASETS:
                status = self.check_dataset_availability(dataset_key)
                if status['is_usable']:
                    return dataset_key
        
        # å¦‚æœä¼˜å…ˆçº§åˆ—è¡¨ä¸­æ²¡æœ‰å¯ç”¨çš„ï¼Œæ£€æŸ¥æ‰€æœ‰æ•°æ®é›†
        for dataset_key in self.DATASETS.keys():
            status = self.check_dataset_availability(dataset_key)
            if status['is_usable']:
                return dataset_key
        
        return None

# åˆ›å»ºå…¨å±€è·¯å¾„é…ç½®å®ä¾‹
PATH_CONFIG = PathConfig()

# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸå§‹çš„OUTPUT_DIRå˜é‡
OUTPUT_DIR = PATH_CONFIG.BASE_OUTPUT_DIR

# ===============================================================================
# å¯¼å…¥å…¶ä»–æ¨¡å—
# ===============================================================================

# Assuming data_loader, config, and plotting_utils are in the same directory (src)
from data_loader import load_effect_sizes, load_neuron_positions
from config import (
    EFFECT_SIZE_THRESHOLD, BEHAVIOR_COLORS, MIXED_BEHAVIOR_COLORS,
    SHOW_BACKGROUND_NEURONS, BACKGROUND_NEURON_COLOR, 
    BACKGROUND_NEURON_SIZE, BACKGROUND_NEURON_ALPHA,
    STANDARD_KEY_NEURON_ALPHA, USE_STANDARD_ALPHA_FOR_UNSHARED_IN_SCHEME_B # New config imports
)
from plotting_utils import (
    plot_single_behavior_activity_map, 
    plot_shared_neurons_map,
    plot_unique_neurons_map,
    plot_combined_9_grid
)
from effect_size_calculator import EffectSizeCalculator, load_and_calculate_effect_sizes

import matplotlib.pyplot as plt
import seaborn as sns

def analyze_effect_sizes(df_effect_sizes_long):
    """
    Analyzes the effect size data (already in long format) to help determine a threshold.
    Prints descriptive statistics and plots a histogram and boxplot.
    Saves plots to the OUTPUT_DIR.
    Assumes df_effect_sizes_long has columns: 'Behavior', 'NeuronID', 'EffectSize'.
    """
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"Created directory: {OUTPUT_DIR}")

    print("Descriptive statistics for effect sizes:")
    # The describe() on the long format will include NeuronID if not careful.
    # We are interested in the distribution of EffectSize values.
    print(df_effect_sizes_long['EffectSize'].describe())

    # Plot histogram
    plt.figure(figsize=(12, 6))
    sns.histplot(data=df_effect_sizes_long, x='EffectSize', hue='Behavior', kde=True, element="step")
    plt.title('Distribution of Effect Sizes by Behavior')
    plt.xlabel('Effect Size')
    plt.ylabel('Frequency')
    plt.grid(axis='y', alpha=0.75)
    hist_path = os.path.join(OUTPUT_DIR, 'effect_size_histogram.png')
    plt.savefig(hist_path)
    print(f"\nHistogram of effect sizes saved to {hist_path}")
    # plt.show()

    # Plot boxplot
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=df_effect_sizes_long, x='Behavior', y='EffectSize')
    plt.title('Box Plot of Effect Sizes by Behavior')
    plt.xlabel('Behavior')
    plt.ylabel('Effect Size')
    plt.grid(axis='y', alpha=0.75)
    box_path = os.path.join(OUTPUT_DIR, 'effect_size_boxplot.png')
    plt.savefig(box_path)
    print(f"Boxplot of effect sizes saved to {box_path}")
    # plt.show()
    
    print("\nConsider the overall distribution, the spread within each behavior,")
    print("and any natural breaks or clusters when choosing a threshold.")
    print("You might want to choose a threshold that captures the upper quartile, for example,")
    print("or a value that seems to separate 'strong' effects from weaker ones based on the plots.")

def suggest_threshold_for_neuron_count(df_effects, min_neurons=5, max_neurons=10):
    print(f"\nAnalyzing effect sizes to find a threshold that yields {min_neurons}-{max_neurons} neurons per behavior.")

    potential_t_values = set()
    # Add effect sizes around the Nth neuron mark as candidates
    for behavior in df_effects['Behavior'].unique():
        behavior_df = df_effects[df_effects['Behavior'] == behavior].copy()
        behavior_df.sort_values(by='EffectSize', ascending=False, inplace=True)
        
        if len(behavior_df) >= min_neurons:
            potential_t_values.add(round(behavior_df['EffectSize'].iloc[min_neurons - 1], 4)) # N_min_th neuron
        if len(behavior_df) > min_neurons -1 and min_neurons > 1 :
            # Add value slightly above (N_min-1)th neuron's ES to catch exactly N_min
            potential_t_values.add(round(behavior_df['EffectSize'].iloc[min_neurons - 2], 4) + 0.00001) 

        if len(behavior_df) >= max_neurons:
            potential_t_values.add(round(behavior_df['EffectSize'].iloc[max_neurons - 1], 4)) # N_max_th neuron
        if len(behavior_df) > max_neurons:
            # Add value slightly above (N_max+1)th neuron's ES to ensure <= N_max neurons
            potential_t_values.add(round(behavior_df['EffectSize'].iloc[max_neurons], 4) + 0.00001)
    
    # Add some generic sensible thresholds
    generic_thresholds = [0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65]
    for gt in generic_thresholds:
        potential_t_values.add(gt)
    
    candidate_thresholds = sorted([val for val in list(potential_t_values) if val > 0])

    best_t = None
    best_t_score = float('inf')
    best_t_counts = {}

    print(f"\nTesting {len(candidate_thresholds)} candidate thresholds...") # ({', '.join(f'{x:.3f}' for x in candidate_thresholds)}) 

    for t in candidate_thresholds:
        current_score_penalty = 0
        counts_for_t = {}
        all_behaviors_in_desired_range = True
        
        for behavior in df_effects['Behavior'].unique():
            behavior_df = df_effects[df_effects['Behavior'] == behavior]
            count = len(behavior_df[behavior_df['EffectSize'] >= t])
            counts_for_t[behavior] = count
            
            if not (min_neurons <= count <= max_neurons):
                all_behaviors_in_desired_range = False
            
            if count < min_neurons:
                current_score_penalty += (min_neurons - count) * 2 # Heavier penalty for too few
            elif count > max_neurons:
                current_score_penalty += (count - max_neurons)
        
        current_full_score = current_score_penalty
        if all_behaviors_in_desired_range:
            # If all counts are in range, prefer solutions that are more 'balanced'
            # (e.g., sum of squared deviations from the midpoint of the desired range)
            mid_point = (min_neurons + max_neurons) / 2.0
            balance_score = sum((c - mid_point)**2 for c in counts_for_t.values())
            current_full_score = balance_score # Override penalty, use balance score for 'good' thresholds
        
        if current_full_score < best_t_score:
            best_t_score = current_full_score
            best_t = t
            best_t_counts = counts_for_t
        elif current_full_score == best_t_score and (best_t is None or t < best_t):
             # Prefer smaller threshold if scores are identical to be slightly more inclusive
            if all_behaviors_in_desired_range == all(min_neurons <= c <= max_neurons for c in best_t_counts.values()): # only if new one is also 'good'
                best_t = t
                best_t_counts = counts_for_t

    if best_t is not None:
        print(f"\nRecommended threshold: T = {best_t:.4f}") # Using 4 decimal places for threshold
        print("Neuron counts for this threshold:")
        all_final_counts_in_range = True
        for b, c in best_t_counts.items():
            print(f"  {b}: {c} neurons")
            if not (min_neurons <= c <= max_neurons):
                all_final_counts_in_range = False
        if not all_final_counts_in_range:
             print(f"  Note: This threshold aims for the best balance, but some behaviors might be slightly outside the {min_neurons}-{max_neurons} range.")
        return best_t
    else:
        print("\nCould not automatically determine a suitable threshold from the candidates.")
        overall_75th = df_effects['EffectSize'].quantile(0.75)
        print(f"The overall 75th percentile of effect sizes is {overall_75th:.4f}. This could be a starting point for manual selection.")
        return None

def get_key_neurons(df_effects, threshold):
    """
    æ ¹æ®æ•ˆåº”é‡é˜ˆå€¼è¯†åˆ«æ¯ç§è¡Œä¸ºçš„å…³é”®ç¥ç»å…ƒ
    
    å‚æ•°:
        df_effects: æ•ˆåº”é‡æ•°æ®DataFrameï¼ŒåŒ…å«Behaviorã€NeuronIDã€EffectSizeåˆ—
        threshold: æ•ˆåº”é‡é˜ˆå€¼
    
    è¿”å›:
        dict: æ¯ç§è¡Œä¸ºå¯¹åº”çš„å…³é”®ç¥ç»å…ƒIDåˆ—è¡¨
    """
    key_neurons_by_behavior = {}
    
    # è¿‡æ»¤æ‰æ— æ•ˆçš„è¡Œä¸ºåç§°ï¼ˆå¦‚nanå€¼ï¼‰
    valid_behaviors = df_effects['Behavior'].dropna().unique()
    
    for behavior in valid_behaviors:
        # è·³è¿‡nanå€¼æˆ–ç©ºå€¼
        if pd.isna(behavior) or behavior == '' or str(behavior).lower() == 'nan':
            continue
            
        behavior_df = df_effects[df_effects['Behavior'] == behavior]
        key_neuron_ids = behavior_df[behavior_df['EffectSize'] >= threshold]['NeuronID'].tolist()
        key_neurons_by_behavior[behavior] = sorted(list(set(key_neuron_ids)))
        print(f"Behavior: {behavior}, Threshold >= {threshold}, Key Neurons ({len(key_neuron_ids)}): {key_neurons_by_behavior[behavior]}")
    
    return key_neurons_by_behavior

def calculate_effect_sizes_from_data(neuron_data_file: str, output_dir: str = None) -> tuple:
    """
    ä»åŸå§‹ç¥ç»å…ƒæ•°æ®æ–‡ä»¶è®¡ç®—æ•ˆåº”é‡
    
    å‚æ•°ï¼š
        neuron_data_file: åŒ…å«ç¥ç»å…ƒæ´»åŠ¨æ•°æ®å’Œè¡Œä¸ºæ ‡ç­¾çš„æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        
    è¿”å›ï¼š
        tuple: (æ•ˆåº”é‡DataFrame (é•¿æ ¼å¼), æ•ˆåº”é‡è®¡ç®—å™¨å®ä¾‹, è®¡ç®—ç»“æœå­—å…¸)
    """
    print(f"\nä»åŸå§‹æ•°æ®è®¡ç®—æ•ˆåº”é‡: {neuron_data_file}")
    
    # å¦‚æœæœªæŒ‡å®šè¾“å‡ºç›®å½•ï¼Œä½¿ç”¨è·¯å¾„é…ç½®çš„é»˜è®¤ç›®å½•
    if output_dir is None:
        output_dir = PATH_CONFIG.BASE_EFFECT_SIZE_OUTPUT_DIR
    
    try:
        # ä½¿ç”¨ä¾¿æ·å‡½æ•°åŠ è½½æ•°æ®å¹¶è®¡ç®—æ•ˆåº”é‡
        results = load_and_calculate_effect_sizes(
            neuron_data_path=neuron_data_file,
            behavior_col=None,  # å‡è®¾è¡Œä¸ºæ ‡ç­¾åœ¨æœ€åä¸€åˆ—
            output_dir=output_dir
        )
        
        # å°†æ•ˆåº”é‡ç»“æœè½¬æ¢ä¸ºé•¿æ ¼å¼DataFrameï¼ˆä¸ç°æœ‰ä»£ç å…¼å®¹ï¼‰
        effect_sizes_dict = results['effect_sizes']
        behavior_labels = results['behavior_labels']
        
        # åˆ›å»ºé•¿æ ¼å¼DataFrame
        long_format_data = []
        for behavior, effect_array in effect_sizes_dict.items():
            for neuron_idx, effect_value in enumerate(effect_array):
                long_format_data.append({
                    'Behavior': behavior,
                    'NeuronID': neuron_idx + 1,  # 1-basedç´¢å¼•
                    'EffectSize': effect_value
                })
        
        df_effect_sizes_long = pd.DataFrame(long_format_data)
        
        print(f"æ•ˆåº”é‡è®¡ç®—å®Œæˆ:")
        print(f"  è¡Œä¸ºç±»åˆ«: {behavior_labels}")
        print(f"  æ•ˆåº”é‡æ•°æ®å½¢çŠ¶: {df_effect_sizes_long.shape}")
        print(f"  è¾“å‡ºæ–‡ä»¶: {results['output_files']['effect_sizes_csv']}")
        
        return df_effect_sizes_long, results['calculator'], results
        
    except Exception as e:
        print(f"ä»åŸå§‹æ•°æ®è®¡ç®—æ•ˆåº”é‡å¤±è´¥: {str(e)}")
        print("å°†å°è¯•ä½¿ç”¨é¢„è®¡ç®—çš„æ•ˆåº”é‡æ•°æ®...")
        return None, None, None

def create_effect_sizes_workflow(raw_data_file: str = None, 
                                precomputed_file: str = None,
                                recalculate: bool = False) -> pd.DataFrame:
    """
    åˆ›å»ºæ•ˆåº”é‡è®¡ç®—å·¥ä½œæµ
    
    å‚æ•°ï¼š
        raw_data_file: åŸå§‹ç¥ç»å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        precomputed_file: é¢„è®¡ç®—çš„æ•ˆåº”é‡æ–‡ä»¶è·¯å¾„
        recalculate: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—æ•ˆåº”é‡
        
    è¿”å›ï¼š
        pd.DataFrame: æ•ˆåº”é‡æ•°æ®ï¼ˆé•¿æ ¼å¼ï¼‰
    """
    print("\n=== æ•ˆåº”é‡è®¡ç®—å·¥ä½œæµ ===")
    
    # å¦‚æœæŒ‡å®šäº†åŸå§‹æ•°æ®æ–‡ä»¶ä¸”éœ€è¦é‡æ–°è®¡ç®—ï¼Œæˆ–è€…æ²¡æœ‰é¢„è®¡ç®—æ–‡ä»¶
    if (raw_data_file and recalculate) or (raw_data_file and not precomputed_file):
        print("ä½¿ç”¨åŸå§‹æ•°æ®è®¡ç®—æ•ˆåº”é‡...")
        df_long, calculator, results = calculate_effect_sizes_from_data(raw_data_file)
        
        if df_long is not None:
            print("æ•ˆåº”é‡è®¡ç®—æˆåŠŸï¼")
            return df_long
        else:
            print("æ•ˆåº”é‡è®¡ç®—å¤±è´¥ï¼Œå°è¯•åŠ è½½é¢„è®¡ç®—æ•°æ®...")
    
    # å°è¯•åŠ è½½é¢„è®¡ç®—çš„æ•ˆåº”é‡æ•°æ®
    if precomputed_file and os.path.exists(precomputed_file):
        print(f"åŠ è½½é¢„è®¡ç®—çš„æ•ˆåº”é‡æ•°æ®: {precomputed_file}")
        try:
            df_long = load_effect_sizes(precomputed_file)
            if df_long is not None:
                print("é¢„è®¡ç®—æ•ˆåº”é‡æ•°æ®åŠ è½½æˆåŠŸï¼")
                return df_long
            else:
                print("é¢„è®¡ç®—æ•ˆåº”é‡æ•°æ®åŠ è½½å¤±è´¥")
        except Exception as e:
            print(f"åŠ è½½é¢„è®¡ç®—æ•ˆåº”é‡æ•°æ®æ—¶å‡ºé”™: {str(e)}")
    
    # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œç”Ÿæˆç¤ºä¾‹æ•°æ®
    print("æ‰€æœ‰æ•°æ®æºéƒ½ä¸å¯ç”¨ï¼Œç”Ÿæˆç¤ºä¾‹æ•ˆåº”é‡æ•°æ®ç”¨äºæ¼”ç¤º...")
    return generate_sample_effect_sizes()

def generate_sample_effect_sizes() -> pd.DataFrame:
    """
    ç”Ÿæˆç¤ºä¾‹æ•ˆåº”é‡æ•°æ®ç”¨äºæ¼”ç¤º
    """
    print("ç”Ÿæˆç¤ºä¾‹æ•ˆåº”é‡æ•°æ®...")
    
    behaviors = ['Close', 'Middle', 'Open']
    n_neurons = 50
    
    # ç”Ÿæˆéšæœºæ•ˆåº”é‡æ•°æ®
    np.random.seed(42)
    long_format_data = []
    
    for behavior in behaviors:
        # ä¸ºæ¯ç§è¡Œä¸ºç”Ÿæˆæ•ˆåº”é‡ï¼Œéƒ¨åˆ†ç¥ç»å…ƒæœ‰è¾ƒé«˜æ•ˆåº”é‡
        effect_sizes = np.random.exponential(scale=0.3, size=n_neurons)
        
        # è®©æŸäº›ç¥ç»å…ƒå¯¹ç‰¹å®šè¡Œä¸ºæœ‰æ›´é«˜çš„æ•ˆåº”é‡
        if behavior == 'Close':
            effect_sizes[0:10] += np.random.uniform(0.4, 0.8, 10)
        elif behavior == 'Middle':
            effect_sizes[15:25] += np.random.uniform(0.4, 0.8, 10)
        else:  # Open
            effect_sizes[30:40] += np.random.uniform(0.4, 0.8, 10)
        
        for neuron_id in range(1, n_neurons + 1):
            long_format_data.append({
                'Behavior': behavior,
                'NeuronID': neuron_id,
                'EffectSize': effect_sizes[neuron_id - 1]
            })
    
    df_sample = pd.DataFrame(long_format_data)
    print(f"ç¤ºä¾‹æ•°æ®ç”Ÿæˆå®Œæˆ: {df_sample.shape}")
    return df_sample

if __name__ == "__main__":
    # ===============================================================================
    # ä¸»ç¨‹åºå…¥å£ - ä½¿ç”¨è·¯å¾„é…ç½®
    # ===============================================================================
    
    print("=" * 80)
    print("ç¥ç»å…ƒä¸»è¦åˆ†æå™¨ - å¤šæ•°æ®é›†æ”¯æŒç‰ˆæœ¬")
    print("=" * 80)
    print(f"è¾“å‡ºç›®å½•: {PATH_CONFIG.BASE_OUTPUT_DIR}")
    print(f"æ•ˆåº”é‡è¾“å‡ºç›®å½•: {PATH_CONFIG.BASE_EFFECT_SIZE_OUTPUT_DIR}")
    
    # ===============================================================================
    # æ•°æ®é›†é€‰æ‹©é…ç½®
    # ===============================================================================
    
    # ğŸ”§ åœ¨è¿™é‡Œä¿®æ”¹ dataset_key æ¥åˆ‡æ¢ä¸åŒçš„æ•°æ®é›†
    # å¯é€‰å€¼: 'emtrace01', 'emtrace02', 'emtrace01_plus', 'emtrace02_plus', 
    #         '2980', 'bla6250', 'day3', 'day6', 'day9'
    # è®¾ç½®ä¸º None ä¼šè‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„æ•°æ®é›†
    
    # dataset_key = None # ğŸ”§ ä¿®æ”¹è¿™é‡Œæ¥æŒ‡å®šæ•°æ®é›†ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
    dataset_key = 'emtrace01'    # ä½¿ç”¨EMtrace01æ•°æ®é›†
    # dataset_key = 'emtrace02'    # ä½¿ç”¨EMtrace02æ•°æ®é›†  
    # dataset_key = '2980'         # ä½¿ç”¨2980æ•°æ®é›†
    # dataset_key = '2980_plus'      # ä½¿ç”¨2980å¢å¼ºç‰ˆæ•°æ®é›†
    # dataset_key = 'bla6250'      # ä½¿ç”¨BLA6250æ•°æ®é›†
    # dataset_key = 'bla6250_plus' # ä½¿ç”¨BLA6250å¢å¼ºç‰ˆæ•°æ®é›†
    # dataset_key = 'day3'         # ä½¿ç”¨Day3æ•°æ®é›†
    
    # ===============================================================================
    # æ™ºèƒ½æ•°æ®é›†é€‰æ‹©å’ŒéªŒè¯
    # ===============================================================================
    
    # æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨æ•°æ®é›†
    print("\n" + "=" * 60)
    print("ğŸ” æ£€æŸ¥å¯ç”¨æ•°æ®é›†...")
    PATH_CONFIG.list_available_datasets()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ•°æ®é›†ï¼Œè‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„æ•°æ®é›†
    if dataset_key is None:
        print("\nğŸ¤– æœªæŒ‡å®šæ•°æ®é›†ï¼Œæ­£åœ¨è‡ªåŠ¨é€‰æ‹©æœ€ä½³å¯ç”¨æ•°æ®é›†...")
        dataset_key = PATH_CONFIG.get_recommended_dataset()
        if dataset_key is None:
            print("âŒ é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯ç”¨çš„æ•°æ®é›†ï¼")
            print("è¯·æ£€æŸ¥dataç›®å½•ä¸­çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚")
            exit(1)
        else:
            print(f"âœ… è‡ªåŠ¨é€‰æ‹©æ•°æ®é›†: {dataset_key}")
    
    # éªŒè¯é€‰æ‹©çš„æ•°æ®é›†
    print(f"\nğŸ” éªŒè¯æ•°æ®é›†: {dataset_key}")
    status = PATH_CONFIG.print_dataset_status(dataset_key)
    
    if not status['is_usable']:
        print(f"\nâŒ é”™è¯¯ï¼šæ•°æ®é›† '{dataset_key}' ä¸å¯ç”¨ï¼")
        print("è¯·é€‰æ‹©å…¶ä»–æ•°æ®é›†æˆ–æ£€æŸ¥æ–‡ä»¶è·¯å¾„ã€‚")
        
        # å°è¯•æ¨èæ›¿ä»£æ•°æ®é›†
        alternative = PATH_CONFIG.get_recommended_dataset()
        if alternative and alternative != dataset_key:
            print(f"\nğŸ’¡ å»ºè®®ä½¿ç”¨æ•°æ®é›†: {alternative}")
            PATH_CONFIG.print_dataset_status(alternative)
        exit(1)
    
    # è·å–å½“å‰æ•°æ®é›†çš„è·¯å¾„é…ç½®
    try:
        data_paths = PATH_CONFIG.get_data_paths(dataset_key)
        raw_data_identifier = data_paths['raw']
        effect_data_identifier = data_paths['effect']
        position_data_identifier = data_paths['position']
        
        print(f"\nâœ… ä½¿ç”¨æ•°æ®é›†: {data_paths['name']} ('{dataset_key}')")
        print(f"ğŸ“„ æè¿°: {data_paths['description']}")
        print(f"ğŸ“ åŸå§‹æ•°æ®æ–‡ä»¶: {raw_data_identifier or 'æ— '}")
        print(f"ğŸ“ æ•ˆåº”é‡æ•°æ®æ–‡ä»¶: {effect_data_identifier or 'éœ€è¦è®¡ç®—'}")
        print(f"ğŸ“ ä½ç½®æ•°æ®æ–‡ä»¶: {position_data_identifier}")
        
    except ValueError as e:
        print(f"âŒ é”™è¯¯: {e}")
        exit(1)

    # === æ•ˆåº”é‡è®¡ç®—å·¥ä½œæµ ===
    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹åˆ†ææµç¨‹")
    print("=" * 60)
    
    # åˆ›å»ºæ•ˆåº”é‡è®¡ç®—å·¥ä½œæµ
    df_effect_sizes_transformed = create_effect_sizes_workflow(
        raw_data_file=raw_data_identifier if raw_data_identifier and os.path.exists(raw_data_identifier) else None,
        precomputed_file=effect_data_identifier if effect_data_identifier and os.path.exists(effect_data_identifier) else None,
        recalculate=False  # è®¾ç½®ä¸ºTrueå¼ºåˆ¶é‡æ–°è®¡ç®—æ•ˆåº”é‡
    )
    
    print(f"\nğŸ“ Loading neuron positions from: {position_data_identifier}")
    df_neuron_positions = load_neuron_positions(position_data_identifier)

    if df_effect_sizes_transformed is not None and df_neuron_positions is not None:
        print(f"\nğŸ¯ Using effect size threshold: {EFFECT_SIZE_THRESHOLD} (from config.py)")
        
        # Get key neurons based on the threshold
        key_neurons_by_behavior = get_key_neurons(df_effect_sizes_transformed, EFFECT_SIZE_THRESHOLD)
        
        # è·å–æ‰€æœ‰æœ‰æ•ˆçš„è¡Œä¸ºåç§°ï¼ˆæ’é™¤nanç­‰æ— æ•ˆå€¼ï¼‰
        all_behaviors = list(key_neurons_by_behavior.keys())
        print(f"\nğŸ“Š å‘ç° {len(all_behaviors)} ä¸ªæœ‰æ•ˆè¡Œä¸ºæ ‡ç­¾: {all_behaviors}")
        
        # ===============================================================================
        # ç”Ÿæˆå•ç‹¬çš„å›¾è¡¨ï¼ˆæ¯ä¸ªè¡Œä¸ºã€æ¯å¯¹è¡Œä¸ºå…±äº«ã€æ¯ä¸ªè¡Œä¸ºç‹¬æœ‰ï¼‰
        # ===============================================================================
        
        print(f"\nğŸ¨ å¼€å§‹ç”Ÿæˆå•ç‹¬çš„å›¾è¡¨...")
        
        # --- 1. ä¸ºæ¯ä¸ªè¡Œä¸ºç”Ÿæˆå…³é”®ç¥ç»å…ƒå›¾ ---
        print(f"\nğŸ“ˆ ç”Ÿæˆæ¯ä¸ªè¡Œä¸ºçš„å…³é”®ç¥ç»å…ƒå›¾...")
        
        for behavior_name in all_behaviors:
            print(f"  ğŸ”¸ ç”Ÿæˆ {behavior_name} è¡Œä¸ºçš„å…³é”®ç¥ç»å…ƒå›¾...")
            
            # è·å–è¯¥è¡Œä¸ºçš„å…³é”®ç¥ç»å…ƒ
            neuron_ids = key_neurons_by_behavior.get(behavior_name, [])
            if not neuron_ids:
                print(f"    âš ï¸  {behavior_name} æ²¡æœ‰å…³é”®ç¥ç»å…ƒï¼Œè·³è¿‡...")
                continue
                
            key_neurons_df = df_neuron_positions[df_neuron_positions['NeuronID'].isin(neuron_ids)]
            
            # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶åï¼ˆå¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
            safe_behavior_name = behavior_name.replace('/', '-').replace('\\', '-').replace(':', '-').replace('*', '-').replace('?', '-').replace('"', '-').replace('<', '-').replace('>', '-').replace('|', '-')
            output_filename = f"behavior_{safe_behavior_name}_key_neurons.png"
            output_path = os.path.join(data_paths['output_dir'], output_filename)
            
            # ä½¿ç”¨ç°æœ‰çš„ç»˜å›¾å‡½æ•°
            try:
                plot_single_behavior_activity_map(
                    key_neurons_df=key_neurons_df,
                    behavior_name=behavior_name,
                    behavior_color=BEHAVIOR_COLORS.get(behavior_name, 'gray'),
                    title=f'{behavior_name} Key Neurons',
                    output_path=output_path,
                    all_neuron_positions_df=df_neuron_positions,
                    show_background_neurons=SHOW_BACKGROUND_NEURONS,
                    background_neuron_color=BACKGROUND_NEURON_COLOR,
                    background_neuron_size=BACKGROUND_NEURON_SIZE,
                    background_neuron_alpha=BACKGROUND_NEURON_ALPHA,
                    key_neuron_size=300,
                    key_neuron_alpha=STANDARD_KEY_NEURON_ALPHA,
                    show_title=True
                )
                print(f"    âœ… ä¿å­˜åˆ°: {output_filename}")
            except Exception as e:
                print(f"    âŒ ç”Ÿæˆ {behavior_name} å›¾è¡¨å¤±è´¥: {str(e)}")
        
        # --- 2. ä¸ºæ¯å¯¹è¡Œä¸ºç”Ÿæˆå…±äº«ç¥ç»å…ƒå›¾ ---
        print(f"\nğŸ”— ç”Ÿæˆæ¯å¯¹è¡Œä¸ºçš„å…±äº«ç¥ç»å…ƒå›¾...")
        
        behavior_pairs = list(combinations(all_behaviors, 2))
        print(f"  ğŸ“Š æ€»å…±éœ€è¦ç”Ÿæˆ {len(behavior_pairs)} ä¸ªå…±äº«ç¥ç»å…ƒå›¾")
        
        for b1, b2 in behavior_pairs:
            print(f"  ğŸ”¸ ç”Ÿæˆ {b1} ä¸ {b2} çš„å…±äº«ç¥ç»å…ƒå›¾...")
            
            # è·å–ä¸¤ä¸ªè¡Œä¸ºçš„å…³é”®ç¥ç»å…ƒé›†åˆ
            ids1 = set(key_neurons_by_behavior.get(b1, []))
            ids2 = set(key_neurons_by_behavior.get(b2, []))
            shared_ids = list(ids1.intersection(ids2))
            
            if not shared_ids:
                print(f"    âš ï¸  {b1} ä¸ {b2} æ²¡æœ‰å…±äº«å…³é”®ç¥ç»å…ƒï¼Œè·³è¿‡...")
                continue
            
            # è·å–æ•°æ®æ¡†
            df_b1_all_key = df_neuron_positions[df_neuron_positions['NeuronID'].isin(list(ids1))]
            df_b2_all_key = df_neuron_positions[df_neuron_positions['NeuronID'].isin(list(ids2))]
            df_shared_key = df_neuron_positions[df_neuron_positions['NeuronID'].isin(shared_ids)]
            
            # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
            safe_b1 = b1.replace('/', '-').replace('\\', '-').replace(':', '-').replace('*', '-').replace('?', '-').replace('"', '-').replace('<', '-').replace('>', '-').replace('|', '-')
            safe_b2 = b2.replace('/', '-').replace('\\', '-').replace(':', '-').replace('*', '-').replace('?', '-').replace('"', '-').replace('<', '-').replace('>', '-').replace('|', '-')
            output_filename = f"shared_{safe_b1}_and_{safe_b2}.png"
            output_path = os.path.join(data_paths['output_dir'], output_filename)
            
            # ä½¿ç”¨ç°æœ‰çš„ç»˜å›¾å‡½æ•°
            try:
                # è·å–æ··åˆé¢œè‰²
                mixed_color_key = tuple(sorted((b1, b2)))
                mixed_color = MIXED_BEHAVIOR_COLORS.get(mixed_color_key, 'purple')
                
                plot_shared_neurons_map(
                    behavior1_name=b1,
                    behavior2_name=b2,
                    behavior1_all_key_neurons_df=df_b1_all_key,
                    behavior2_all_key_neurons_df=df_b2_all_key,
                    shared_key_neurons_df=df_shared_key,
                    color1=BEHAVIOR_COLORS.get(b1, 'pink'),
                    color2=BEHAVIOR_COLORS.get(b2, 'lightblue'),
                    mixed_color=mixed_color,
                    title=f'{b1}-{b2} Shared Neurons',
                    output_path=output_path,
                    all_neuron_positions_df=df_neuron_positions,
                    scheme='B',  # ä½¿ç”¨æ–¹æ¡ˆB
                    show_background_neurons=SHOW_BACKGROUND_NEURONS,
                    background_neuron_color=BACKGROUND_NEURON_COLOR,
                    background_neuron_size=BACKGROUND_NEURON_SIZE,
                    background_neuron_alpha=BACKGROUND_NEURON_ALPHA,
                    show_title=True,
                    standard_key_neuron_alpha=STANDARD_KEY_NEURON_ALPHA,
                    use_standard_alpha_for_unshared_in_scheme_b=USE_STANDARD_ALPHA_FOR_UNSHARED_IN_SCHEME_B,
                    alpha_non_shared=0.3,
                    shared_marker_size_factor=1.5
                )
                print(f"    âœ… ä¿å­˜åˆ°: {output_filename} (å…±äº«ç¥ç»å…ƒæ•°: {len(shared_ids)})")
            except Exception as e:
                print(f"    âŒ ç”Ÿæˆ {b1}-{b2} å…±äº«å›¾è¡¨å¤±è´¥: {str(e)}")
        
        # --- 3. ä¸ºæ¯ä¸ªè¡Œä¸ºç”Ÿæˆç‹¬æœ‰ç¥ç»å…ƒå›¾ ---
        print(f"\nğŸ¯ ç”Ÿæˆæ¯ä¸ªè¡Œä¸ºçš„ç‹¬æœ‰ç¥ç»å…ƒå›¾...")
        
        # è®¡ç®—æ¯ä¸ªè¡Œä¸ºçš„ç‹¬æœ‰ç¥ç»å…ƒ
        all_behavior_sets = {name: set(key_neurons_by_behavior.get(name, [])) for name in all_behaviors}
        
        for behavior_name in all_behaviors:
            print(f"  ğŸ”¸ ç”Ÿæˆ {behavior_name} è¡Œä¸ºçš„ç‹¬æœ‰ç¥ç»å…ƒå›¾...")
            
            # è·å–è¯¥è¡Œä¸ºçš„ç¥ç»å…ƒé›†åˆ
            current_behavior_neurons = all_behavior_sets.get(behavior_name, set())
            
            # è·å–å…¶ä»–æ‰€æœ‰è¡Œä¸ºçš„ç¥ç»å…ƒé›†åˆ
            other_behaviors_neurons = set()
            for other_name in all_behaviors:
                if other_name != behavior_name:
                    other_behaviors_neurons.update(all_behavior_sets.get(other_name, set()))
            
            # è®¡ç®—ç‹¬æœ‰ç¥ç»å…ƒ
            unique_ids = list(current_behavior_neurons - other_behaviors_neurons)
            
            if not unique_ids:
                print(f"    âš ï¸  {behavior_name} æ²¡æœ‰ç‹¬æœ‰å…³é”®ç¥ç»å…ƒï¼Œè·³è¿‡...")
                continue
            
            unique_neurons_df = df_neuron_positions[df_neuron_positions['NeuronID'].isin(unique_ids)]
            
            # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
            safe_behavior_name = behavior_name.replace('/', '-').replace('\\', '-').replace(':', '-').replace('*', '-').replace('?', '-').replace('"', '-').replace('<', '-').replace('>', '-').replace('|', '-')
            output_filename = f"unique_{safe_behavior_name}_neurons.png"
            output_path = os.path.join(data_paths['output_dir'], output_filename)
            
            # ä½¿ç”¨ç°æœ‰çš„ç»˜å›¾å‡½æ•°
            try:
                plot_unique_neurons_map(
                    unique_neurons_df=unique_neurons_df,
                    behavior_name=behavior_name,
                    behavior_color=BEHAVIOR_COLORS.get(behavior_name, 'gray'),
                    title=f'{behavior_name} Unique Neurons',
                    output_path=output_path,
                    all_neuron_positions_df=df_neuron_positions,
                    show_background_neurons=SHOW_BACKGROUND_NEURONS,
                    background_neuron_color=BACKGROUND_NEURON_COLOR,
                    background_neuron_size=BACKGROUND_NEURON_SIZE,
                    background_neuron_alpha=BACKGROUND_NEURON_ALPHA,
                    key_neuron_size=300,
                    key_neuron_alpha=STANDARD_KEY_NEURON_ALPHA,
                    show_title=True
                )
                print(f"    âœ… ä¿å­˜åˆ°: {output_filename} (ç‹¬æœ‰ç¥ç»å…ƒæ•°: {len(unique_ids)})")
            except Exception as e:
                print(f"    âŒ ç”Ÿæˆ {behavior_name} ç‹¬æœ‰å›¾è¡¨å¤±è´¥: {str(e)}")
        
        # ===============================================================================
        # ç”Ÿæˆç»Ÿè®¡æ±‡æ€»
        # ===============================================================================
        
        print(f"\nğŸ“Š ç”Ÿæˆç»Ÿè®¡æ±‡æ€»...")
        total_individual_plots = len([b for b in all_behaviors if key_neurons_by_behavior.get(b, [])])
        total_shared_plots = len([pair for pair in behavior_pairs if set(key_neurons_by_behavior.get(pair[0], [])).intersection(set(key_neurons_by_behavior.get(pair[1], [])))])
        total_unique_plots = len([b for b in all_behaviors if list(set(key_neurons_by_behavior.get(b, [])) - set().union(*[set(key_neurons_by_behavior.get(other, [])) for other in all_behaviors if other != b]))])
        
        print(f"  ğŸ“ˆ ä¸ªä½“è¡Œä¸ºå›¾è¡¨: {total_individual_plots} å¼ ")
        print(f"  ğŸ”— å…±äº«ç¥ç»å…ƒå›¾è¡¨: {total_shared_plots} å¼ ") 
        print(f"  ğŸ¯ ç‹¬æœ‰ç¥ç»å…ƒå›¾è¡¨: {total_unique_plots} å¼ ")
        print(f"  ğŸ“¦ æ€»è®¡å›¾è¡¨æ•°é‡: {total_individual_plots + total_shared_plots + total_unique_plots} å¼ ")

        print("\nâœ… All plots generated successfully!")
        print(f"ğŸ“ Output directory: {data_paths['output_dir']}")

    else:
        if df_effect_sizes_transformed is None:
            print("âŒ Could not load effect sizes. Please check the effect size data file.")
        if df_neuron_positions is None:
            print("âŒ Could not load neuron positions. Please check the position data file.")

    print("\n" + "=" * 80)
    print("ğŸ‰ Analysis completed!")
    print("=" * 80)