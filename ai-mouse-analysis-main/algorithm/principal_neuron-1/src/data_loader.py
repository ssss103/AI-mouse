# This file will contain functions to load and preprocess data.

import pandas as pd
import io

def load_effect_sizes(csv_path_or_data):
    """加载神经元效应大小数据。

    参数:
        csv_path_or_data (str 或 pd.DataFrame): CSV文件路径或已加载的DataFrame。
                                                如果为路径字符串，且包含 '.csv'，则尝试读取。
                                                当前版本为演示目的，若特定文件名匹配，则使用内置的硬编码数据。

    返回:
        pd.DataFrame: 包含 'Behavior', 'NeuronID', 'EffectSize' 列的整理后的长格式DataFrame。
                      如果加载失败则返回 None 或抛出异常。
    
    异常:
        FileNotFoundError: 如果提供了路径但找不到文件，且无硬编码数据匹配时抛出。
        ValueError: 如果输入类型不符合预期（既非有效路径/DataFrame，也非可识别的硬编码键）。
    """
    if isinstance(csv_path_or_data, str) and ".csv" in csv_path_or_data:
        # 首先尝试从真实文件路径读取
        try:
            df = pd.read_csv(csv_path_or_data)
            print(f"成功从文件加载效应量数据: {csv_path_or_data}")
        except FileNotFoundError:
            print(f"警告: 在 load_effect_sizes 中为 {csv_path_or_data} 使用硬编码的CSV数据。")
            if 'EMtrace01-3标签版.csv' in csv_path_or_data: 
                csv_data_effect_sizes = """Behavior,Neuron_3,Neuron_25,Neuron_42,Neuron_32,Neuron_39,Neuron_41,Neuron_9,Neuron_31,Neuron_19,Neuron_11,Neuron_20,Neuron_34,Neuron_7,Neuron_13,Neuron_18,Neuron_1,Neuron_26,Neuron_16,Neuron_17,Neuron_5,Neuron_2,Neuron_10,Neuron_12,Neuron_36,Neuron_14,Neuron_33,Neuron_35,Neuron_28,Neuron_23,Neuron_40,Neuron_27,Neuron_6,Neuron_4,Neuron_24,Neuron_43,Neuron_37,Neuron_30,Neuron_29,Neuron_38,Neuron_8,Neuron_15,Neuron_22,Neuron_21\nClose,0.758557937108231,0.6720367348396317,0.6494321208505837,0.5724097605190993,0.5575355633362508,0.49138577462626176,0.46495758255985253,0.4544997968460431,0.44464426590169176,0.43487171094008764,0.40832738214265546,0.4071089788765183,0.3895054982861908,0.3685262925005171,0.3557133952168657,0.34997083119669725,0.3494054370223434,0.3232862048041075,0.3202742633469904,0.3184445265754924,0.30729064274122553,0.300070424103099,0.2796847924664112,0.2640809983372798,0.2572234776239322,0.2565768473382203,0.24198602847945683,0.22691315928912564,0.22450216049101196,0.20997100528969895,0.18209991361267383,0.17184710064546443,0.12669533785356132,0.07709502669579277,0.06972428329083036,0.06077470435941399,0.057740914493420004,0.04138112533370128,0.030531706992371024,0.022185666585002237,0.015447357162269915,0.0020497861720446884,0.0013513212151392946\nMiddle,0.8810419960894504,0.6185626854454782,0.8461249329775838,0.40603222723167853,0.10410784592237876,0.43759215462845624,0.40645890896187153,0.4238540819920394,0.5721988017022529,0.26739039303241724,0.5485763912643815,0.28280281018869086,0.45456215907085656,0.5910569446726252,0.6049221754613747,0.41568216488244,0.18396951438615533,0.10435748142236774,0.539836635189018,0.34498804529693383,0.2236505023038027,0.20636413120526895,0.1976311024147914,0.11625066533331936,0.36835468243135416,0.37938171262156606,0.36843271792333016,0.20388001022073926,0.12853152937574364,0.440692920692789,0.09811891314870023,0.37950481302752814,0.3941218365118574,0.15394677255404748,0.4589449000346,0.20488161794478077,0.369190077559868,0.23661157968121782,0.33328815887572827,0.09605007950790909,0.18258865686392428,0.2802159893623729,0.16253856349403137\nOpen,0.005131690021005813,0.27430411019535705,0.21622016142000955,0.48505974736839663,0.8911069952990891,0.1391105068967603,0.19813854434059588,0.14709373085072375,0.3645501294877545,0.42387477695015946,0.2611996382202606,0.30544891675570085,0.021441828710375664,0.3723839129910263,0.6053273983295515,0.03616208229116624,0.3721044829051708,0.5076242221996276,0.4606866485109019,1.3427023637796562,0.2631856525692737,0.7062265471156847,0.18262191211740256,0.3140969617372082,0.17215830317442965,0.24323484610874002,0.15816297135553106,0.08563982891423427,0.20132878837770685,0.5025303257606805,0.5667411140981464,0.3506976325270987,0.4141290162541595,0.15929465677344223,0.5234442149865232,0.23625909502353107,0.7259663051484502,0.37987151477878084,0.42771365855984195,0.1947149846276125,0.3163269733224369,0.606993420979607,0.36874445689264196"""
                df = pd.read_csv(io.StringIO(csv_data_effect_sizes))
            else:
                # 在实际项目中，如果文件未找到且无硬编码数据，应抛出 FileNotFoundError
                raise FileNotFoundError(f"无法从磁盘读取文件 {csv_path_or_data}，且没有匹配的硬编码数据。")
    elif isinstance(csv_path_or_data, pd.DataFrame):
        df = csv_path_or_data # 如果已经是DataFrame，直接使用
    else:
        raise ValueError("输入必须是CSV文件路径或pandas DataFrame。")
    
    # 检查数据格式并进行相应处理
    print(f"效应量数据形状: {df.shape}")
    print(f"列名: {list(df.columns)}")
    
    # 检查是否包含 'Behavior' 列
    if 'Behavior' not in df.columns:
        raise ValueError("效应量数据必须包含 'Behavior' 列")
    
    # 获取神经元列（除了 'Behavior' 列之外的所有列）
    neuron_columns = [col for col in df.columns if col != 'Behavior']
    
    # 检查神经元列格式
    if not neuron_columns:
        raise ValueError("未找到神经元数据列")
    
    # 检查神经元列是否以 'Neuron_' 开头
    if not all(col.startswith('Neuron_') for col in neuron_columns):
        # 如果不是标准格式，尝试转换
        print("警告: 神经元列不是标准格式 (Neuron_X)，尝试转换...")
        # 这里可以添加其他格式的处理逻辑
        raise ValueError(f"神经元列格式不正确。期望格式: 'Neuron_X'，实际格式: {neuron_columns[:5]}...")
    
    # 将数据转换为长格式: Behavior, NeuronID, EffectSize
    df_melted = df.melt(id_vars=['Behavior'], var_name='Neuron_str', value_name='EffectSize')
    
    # 从 'Neuron_X' 提取 NeuronID (整数)
    try:
        df_melted['NeuronID'] = df_melted['Neuron_str'].str.replace('Neuron_', '').astype(int)
    except ValueError as e:
        print(f"错误: 无法从神经元列名提取ID: {e}")
        print(f"神经元列样例: {df_melted['Neuron_str'].head().tolist()}")
        raise
    
    # 返回处理后的数据
    result_df = df_melted[['Behavior', 'NeuronID', 'EffectSize']]
    
    print(f"转换后的长格式数据形状: {result_df.shape}")
    print(f"包含的行为: {result_df['Behavior'].unique().tolist()}")
    print(f"神经元数量: {result_df['NeuronID'].nunique()}")
    
    return result_df

def load_neuron_positions(csv_path_or_data):
    """加载神经元位置数据。

    参数:
        csv_path_or_data (str 或 pd.DataFrame): CSV文件路径或已加载的DataFrame。
                                                逻辑同 load_effect_sizes。

    返回:
        pd.DataFrame: 包含 'NeuronID', 'x', 'y' 列的DataFrame。
                      如果加载失败则返回 None 或抛出异常。
    """
    if isinstance(csv_path_or_data, str) and ".csv" in csv_path_or_data:
        # 首先尝试从真实文件路径读取
        try:
            df = pd.read_csv(csv_path_or_data)
            print(f"成功从文件加载神经元位置数据: {csv_path_or_data}")
        except FileNotFoundError:
            print(f"警告: 在 load_neuron_positions 中为 {csv_path_or_data} 使用硬编码的CSV数据。")
            if 'EMtrace01_Max_position.csv' in csv_path_or_data:
                csv_data_positions = """number,relative_x,relative_y\n1.000000000000000000e+00,5.629494067083995468e-01,1.688556607751831029e-01\n2.000000000000000000e+00,4.900548466607049791e-01,2.315884059334164524e-01\n3.000000000000000000e+00,4.536075666368575843e-01,2.556137125897611773e-01\n4.000000000000000000e+00,3.157912890466847422e-01,3.103380221958795815e-01\n5.000000000000000000e+00,5.048615541703930232e-01,3.450412429217107890e-01\n6.000000000000000000e+00,6.836810217873939832e-01,4.251255984428596868e-01\n7.000000000000000000e+00,3.829909615906533782e-01,5.012057361879512785e-01\n8.000000000000000000e+00,6.050915742359730398e-01,4.945320398945221019e-01\n9.000000000000000000e+00,3.442657265653154552e-01,5.786206131917285278e-01\n1.000000000000000000e+01,6.996267067978272358e-01,6.013111805893873729e-01\n1.100000000000000000e+01,6.381219217575848646e-01,6.146585731762456151e-01\n1.200000000000000000e+01,5.470037216979662942e-01,6.480270546433909429e-01\n1.300000000000000000e+01,4.501906341346218476e-01,7.040861035081952268e-01\n1.400000000000000000e+01,5.128343966756094829e-01,7.054208427668811066e-01\n1.500000000000000000e+01,3.533775465712773456e-01,3.797444636475419966e-01\n1.600000000000000000e+01,4.000756241018317838e-01,3.463759821803965577e-01\n1.700000000000000000e+01,2.702321890168754570e-01,4.785151687902923223e-01\n1.800000000000000000e+01,4.137433541107744528e-01,7.054208427668811066e-01\n1.900000000000000000e+01,5.458647441972210856e-01,3.984308132691434245e-01\n2.000000000000000000e+01,5.937017992285208434e-01,3.650623318019979857e-01\n2.100000000000000000e+01,5.230851941823165818e-01,4.504856443578901803e-01\n2.200000000000000000e+01,5.834510017218137445e-01,5.639384813461845170e-01\n2.300000000000000000e+01,5.891458892255397872e-01,7.134292783189959408e-01\n2.400000000000000000e+01,6.381219217575848646e-01,4.384729910297179289e-01\n2.500000000000000000e+01,7.554366043343435644e-01,4.785151687902923223e-01\n2.600000000000000000e+01,6.916538642926106650e-01,5.412479139485256718e-01\n2.700000000000000000e+01,6.278711242508777657e-01,2.329231451921022211e-01\n2.800000000000000000e+01,8.271921868812929235e-01,5.812900917091001762e-01\n2.900000000000000000e+01,6.950707967948462906e-01,3.183464577479944158e-01\n3.000000000000000000e+01,8.431378718917261761e-01,3.957613347517717761e-01\n3.100000000000000000e+01,8.078295693686239343e-01,5.279005213616675407e-01\n3.200000000000000000e+01,6.722912467799416758e-01,9.411026228877750199e-02\n3.300000000000000000e+01,7.360739868216745752e-01,1.181355689451220048e-01\n3.400000000000000000e+01,7.520196718321078277e-01,2.249147096399873869e-01\n3.500000000000000000e+01,8.032736593656432111e-01,2.729653229526767255e-01\n3.600000000000000000e+01,8.260532093805477150e-01,2.102325777944433760e-01\n3.700000000000000000e+01,6.802640892851582466e-01,3.677318103193696341e-01\n3.800000000000000000e+01,3.670452765802201811e-01,2.142367955705007931e-01\n3.900000000000000000e+01,4.649973416443098362e-01,6.400186190912761086e-01\n4.000000000000000000e+01,4.980276891659214944e-01,7.748272842185435216e-01\n4.100000000000000000e+01,5.470037216979662942e-01,8.495726827049491225e-01\n4.200000000000000000e+01,7.144334143075151689e-01,7.548061953382562139e-01\n4.300000000000000000e+01,2.565644590079328435e-01,3.450412429217107890e-01"""
                df = pd.read_csv(io.StringIO(csv_data_positions))
            else:
                raise FileNotFoundError(f"无法从磁盘读取文件 {csv_path_or_data}，且没有匹配的硬编码数据。")
    elif isinstance(csv_path_or_data, pd.DataFrame):
        df = csv_path_or_data
    else:
        raise ValueError("输入必须是CSV文件路径或pandas DataFrame。")
    
    df.rename(columns={'number': 'NeuronID', 'relative_x': 'x', 'relative_y': 'y'}, inplace=True)
    df['NeuronID'] = df['NeuronID'].astype(int)
    return df[['NeuronID', 'x', 'y']]


if __name__ == '__main__':
    # 此处的示例用法主要用于直接测试本模块，在主流程中不直接执行。
    # 假设CSV文件位于相对于此脚本的 '../data/' 子目录中 (如果脚本在 src/ 中)
    # 为演示目的，我们将使用硬编码数据的文件名作为键来触发加载。
    effect_data_path_key = 'data/EMtrace01-3标签版.csv'  # 实际使用时应为真实路径
    position_data_path_key = 'data/EMtrace01_Max_position.csv' # 实际使用时应为真实路径

    try:
        df_effects = load_effect_sizes(effect_data_path_key)
        print("\n效应大小数据已加载并转换:")
        print(df_effects.head())
        print(f"总效应大小条目数: {len(df_effects)}")
        print(f"效应数据中不重复的神经元ID数量: {df_effects['NeuronID'].nunique()}")
        print(f"效应数据中的行为类别: {df_effects['Behavior'].unique()}")

        df_positions = load_neuron_positions(position_data_path_key)
        print("\n神经元位置数据已加载:")
        print(df_positions.head())
        print(f"总位置条目数: {len(df_positions)}")
        print(f"位置数据中不重复的神经元ID数量: {df_positions['NeuronID'].nunique()}")

        # 合并数据示例 (通常在主分析脚本中执行此类操作)
        close_effects = df_effects[df_effects['Behavior'] == 'Close']
        merged_close_data = pd.merge(close_effects, df_positions, on='NeuronID', how='inner')
        print("\n合并后的 'Close' 行为数据 (前5条):")
        print(merged_close_data.head())

    except Exception as e:
        print(f"示例用法执行出错: {e}") 