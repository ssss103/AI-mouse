import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

class ResearchMethodologyAdvisor:
    """
    åŸºäºç¥ç»å…ƒæ´»åŠ¨æ¨¡å¼çš„ç ”ç©¶æ–¹æ³•å»ºè®®ç³»ç»Ÿ
    """
    
    def __init__(self, temporal_patterns, clustering_result, response_patterns):
        """
        åˆå§‹åŒ–ç ”ç©¶æ–¹æ³•å»ºè®®å™¨
        
        Args:
            temporal_patterns: æ—¶é—´æ¨¡å¼åˆ†æç»“æœ
            clustering_result: èšç±»åˆ†æç»“æœ
            response_patterns: å“åº”æ¨¡å¼è¯†åˆ«ç»“æœ
        """
        self.temporal_patterns = temporal_patterns
        self.clustering_result = clustering_result
        self.response_patterns = response_patterns
        
    def analyze_data_characteristics(self, df):
        """åˆ†ææ•°æ®ç‰¹å¾ä»¥æŒ‡å¯¼ç ”ç©¶æ–¹æ³•é€‰æ‹©"""
        print("=== æ•°æ®ç‰¹å¾åˆ†æ ===")
        
        characteristics = {}
        
        # åŸºæœ¬æ•°æ®ç‰¹å¾
        n_timepoints = len(df)
        n_neurons = len([col for col in df.columns if col.startswith('Neuron_')])
        n_behaviors = len(df['Behavior'].unique())
        
        print(f"æ•°æ®ç»´åº¦: {n_timepoints} æ—¶é—´ç‚¹ Ã— {n_neurons} ç¥ç»å…ƒ")
        print(f"è¡Œä¸ºç±»å‹æ•°é‡: {n_behaviors}")
        
        # æ•°æ®å®Œæ•´æ€§
        missing_data = df.isnull().sum().sum()
        print(f"ç¼ºå¤±æ•°æ®ç‚¹: {missing_data}")
        
        # æ—¶é—´åºåˆ—ç‰¹å¾
        behavior_sequence_length = n_timepoints
        behavior_transitions = n_timepoints - 1
        
        print(f"åºåˆ—é•¿åº¦: {behavior_sequence_length}")
        print(f"è¡Œä¸ºè½¬æ¢æ¬¡æ•°: {behavior_transitions}")
        
        # ç¥ç»å…ƒæ´»åŠ¨ç‰¹å¾
        neuron_cols = [col for col in df.columns if col.startswith('Neuron_')]
        activity_data = df[neuron_cols]
        
        mean_activity = activity_data.mean().mean()
        std_activity = activity_data.std().mean()
        activity_range = activity_data.max().max() - activity_data.min().min()
        
        print(f"å¹³å‡ç¥ç»å…ƒæ´»åŠ¨: {mean_activity:.3f}")
        print(f"æ´»åŠ¨æ ‡å‡†å·®: {std_activity:.3f}")
        print(f"æ´»åŠ¨èŒƒå›´: {activity_range:.3f}")
        
        characteristics = {
            'n_timepoints': n_timepoints,
            'n_neurons': n_neurons,
            'n_behaviors': n_behaviors,
            'missing_data': missing_data,
            'sequence_length': behavior_sequence_length,
            'transitions': behavior_transitions,
            'mean_activity': mean_activity,
            'std_activity': std_activity,
            'activity_range': activity_range
        }
        
        return characteristics
    
    def suggest_research_paradigm(self, characteristics):
        """åŸºäºæ•°æ®ç‰¹å¾å»ºè®®ç ”ç©¶èŒƒå¼"""
        print("\n=== ç ”ç©¶èŒƒå¼å»ºè®® ===")
        
        paradigms = []
        
        # 1. äº‹ä»¶ç›¸å…³åˆ†æèŒƒå¼
        if characteristics['transitions'] > 5:
            paradigms.append({
                'name': 'äº‹ä»¶ç›¸å…³åˆ†æ (Event-Related Analysis)',
                'description': 'åˆ†æç‰¹å®šè¡Œä¸ºäº‹ä»¶å‰åçš„ç¥ç»å…ƒæ´»åŠ¨å˜åŒ–',
                'approach': [
                    'å®šä¹‰å…³é”®è¡Œä¸ºäº‹ä»¶ï¼ˆå¦‚è¡Œä¸ºè½¬æ¢æ—¶åˆ»ï¼‰',
                    'æå–äº‹ä»¶å‰åå›ºå®šæ—¶é—´çª—å†…çš„ç¥ç»å…ƒæ´»åŠ¨',
                    'è®¡ç®—äº‹ä»¶ç›¸å…³ç”µä½(ERP)æˆ–äº‹ä»¶ç›¸å…³å»åŒæ­¥åŒ–(ERD)',
                    'ç»Ÿè®¡åˆ†æä¸åŒäº‹ä»¶ç±»å‹çš„ç¥ç»ååº”å·®å¼‚'
                ],
                'advantages': 'èƒ½å¤Ÿç²¾ç¡®å®šä½è¡Œä¸ºç›¸å…³çš„ç¥ç»æ´»åŠ¨',
                'suitable_for': 'ç ”ç©¶ç‰¹å®šè¡Œä¸ºçš„ç¥ç»æœºåˆ¶'
            })
        
        # 2. çŠ¶æ€ç©ºé—´åˆ†æèŒƒå¼
        if characteristics['n_behaviors'] >= 3:
            paradigms.append({
                'name': 'çŠ¶æ€ç©ºé—´åˆ†æ (State Space Analysis)',
                'description': 'å°†è¡Œä¸ºå’Œç¥ç»æ´»åŠ¨å»ºæ¨¡ä¸ºåŠ¨æ€ç³»ç»Ÿçš„çŠ¶æ€è½¬æ¢',
                'approach': [
                    'å®šä¹‰è¡Œä¸ºçŠ¶æ€å’Œç¥ç»çŠ¶æ€',
                    'å»ºç«‹çŠ¶æ€è½¬æ¢çŸ©é˜µ',
                    'ä½¿ç”¨éšé©¬å°”å¯å¤«æ¨¡å‹æˆ–å¡å°”æ›¼æ»¤æ³¢',
                    'åˆ†æçŠ¶æ€è½¬æ¢çš„æ¦‚ç‡å’Œç¥ç»åŸºç¡€'
                ],
                'advantages': 'èƒ½å¤Ÿæ•æ‰ç³»ç»Ÿçš„åŠ¨æ€ç‰¹æ€§å’Œé¢„æµ‹æ€§',
                'suitable_for': 'ç ”ç©¶è¡Œä¸ºå†³ç­–å’ŒçŠ¶æ€åˆ‡æ¢æœºåˆ¶'
            })
        
        # 3. ç½‘ç»œè¿æ¥åˆ†æèŒƒå¼
        if characteristics['n_neurons'] >= 20:
            paradigms.append({
                'name': 'åŠŸèƒ½ç½‘ç»œåˆ†æ (Functional Network Analysis)',
                'description': 'åˆ†æç¥ç»å…ƒé—´çš„åŠŸèƒ½è¿æ¥å’Œç½‘ç»œç‰¹æ€§',
                'approach': [
                    'è®¡ç®—ç¥ç»å…ƒé—´çš„åŠŸèƒ½è¿æ¥ï¼ˆç›¸å…³æ€§ã€ç›¸å¹²æ€§ç­‰ï¼‰',
                    'æ„å»ºåŠŸèƒ½ç½‘ç»œå›¾',
                    'åˆ†æç½‘ç»œæ‹“æ‰‘ç‰¹å¾ï¼ˆèšç±»ç³»æ•°ã€è·¯å¾„é•¿åº¦ç­‰ï¼‰',
                    'ç ”ç©¶ä¸åŒè¡Œä¸ºçŠ¶æ€ä¸‹çš„ç½‘ç»œé‡ç»„'
                ],
                'advantages': 'æ­ç¤ºç¥ç»ç½‘ç»œçš„ç»„ç»‡åŸç†å’Œä¿¡æ¯æµ',
                'suitable_for': 'ç ”ç©¶ç¥ç»ç½‘ç»œçš„åŠŸèƒ½æ¶æ„'
            })
        
        # 4. æœºå™¨å­¦ä¹ é¢„æµ‹èŒƒå¼
        if characteristics['sequence_length'] > 10:
            paradigms.append({
                'name': 'é¢„æµ‹æ€§å»ºæ¨¡ (Predictive Modeling)',
                'description': 'ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹è¡Œä¸ºæˆ–ç¥ç»çŠ¶æ€',
                'approach': [
                    'ç‰¹å¾å·¥ç¨‹ï¼šæå–æ—¶é—´ã€é¢‘ç‡ã€ç»Ÿè®¡ç‰¹å¾',
                    'æ¨¡å‹é€‰æ‹©ï¼šæ”¯æŒå‘é‡æœºã€éšæœºæ£®æ—ã€ç¥ç»ç½‘ç»œ',
                    'æ—¶é—´åºåˆ—å»ºæ¨¡ï¼šLSTMã€GRUç­‰å¾ªç¯ç¥ç»ç½‘ç»œ',
                    'äº¤å‰éªŒè¯å’Œæ€§èƒ½è¯„ä¼°'
                ],
                'advantages': 'é‡åŒ–ç¥ç»æ´»åŠ¨å¯¹è¡Œä¸ºçš„é¢„æµ‹èƒ½åŠ›',
                'suitable_for': 'ç ”ç©¶ç¥ç»ç¼–ç å’Œè§£ç æœºåˆ¶'
            })
        
        # æ‰“å°å»ºè®®çš„ç ”ç©¶èŒƒå¼
        for i, paradigm in enumerate(paradigms, 1):
            print(f"\n{i}. {paradigm['name']}")
            print(f"   æè¿°: {paradigm['description']}")
            print("   å®æ–½æ–¹æ³•:")
            for step in paradigm['approach']:
                print(f"     â€¢ {step}")
            print(f"   ä¼˜åŠ¿: {paradigm['advantages']}")
            print(f"   é€‚ç”¨äº: {paradigm['suitable_for']}")
        
        return paradigms
    
    def suggest_specific_models(self, characteristics):
        """æ¨èå…·ä½“çš„åˆ†ææ¨¡å‹å’Œå‚æ•°"""
        print("\n=== å…·ä½“æ¨¡å‹å»ºè®® ===")
        
        models = []
        
        # 1. æ—¶é—´åºåˆ—æ¨¡å‹
        models.append({
            'category': 'æ—¶é—´åºåˆ—åˆ†ææ¨¡å‹',
            'models': [
                {
                    'name': 'ARIMAæ¨¡å‹',
                    'parameters': {
                        'p': '1-3 (è‡ªå›å½’é˜¶æ•°)',
                        'd': '0-1 (å·®åˆ†æ¬¡æ•°)',
                        'q': '1-3 (ç§»åŠ¨å¹³å‡é˜¶æ•°)'
                    },
                    'code_example': '''
from statsmodels.tsa.arima.model import ARIMA
# å¯¹æ¯ä¸ªç¥ç»å…ƒå»ºç«‹ARIMAæ¨¡å‹
model = ARIMA(neuron_activity, order=(2,1,2))
fitted_model = model.fit()
forecast = fitted_model.forecast(steps=5)
                    ''',
                    'interpretation': 'åˆ†æç¥ç»å…ƒæ´»åŠ¨çš„æ—¶é—´ä¾èµ–æ€§å’Œå‘¨æœŸæ€§'
                },
                {
                    'name': 'éšé©¬å°”å¯å¤«æ¨¡å‹(HMM)',
                    'parameters': {
                        'n_states': '3-7 (éšè—çŠ¶æ€æ•°)',
                        'covariance_type': 'full, diag, spherical',
                        'n_iter': '100-1000 (è¿­ä»£æ¬¡æ•°)'
                    },
                    'code_example': '''
from hmmlearn import hmm
# å¤šå…ƒé«˜æ–¯HMM
model = hmm.GaussianHMM(n_components=5, covariance_type="full")
model.fit(neuron_data.values)
hidden_states = model.predict(neuron_data.values)
                    ''',
                    'interpretation': 'è¯†åˆ«æ½œåœ¨çš„ç¥ç»æ´»åŠ¨çŠ¶æ€å’ŒçŠ¶æ€è½¬æ¢è§„å¾‹'
                }
            ]
        })
        
        # 2. èšç±»åˆ†ææ¨¡å‹
        models.append({
            'category': 'èšç±»ä¸é™ç»´æ¨¡å‹',
            'models': [
                {
                    'name': 'è°±èšç±»',
                    'parameters': {
                        'n_clusters': f'{max(3, characteristics["n_neurons"]//10)}-{characteristics["n_neurons"]//5}',
                        'affinity': 'rbf, nearest_neighbors',
                        'gamma': '0.1-10.0'
                    },
                    'code_example': '''
from sklearn.cluster import SpectralClustering
clustering = SpectralClustering(n_clusters=5, affinity='rbf', gamma=1.0)
cluster_labels = clustering.fit_predict(correlation_matrix)
                    ''',
                    'interpretation': 'åŸºäºç›¸ä¼¼æ€§å‘ç°ç¥ç»å…ƒåŠŸèƒ½æ¨¡å—'
                },
                {
                    'name': 'UMAPé™ç»´',
                    'parameters': {
                        'n_neighbors': '5-50',
                        'min_dist': '0.1-0.5',
                        'n_components': '2-10'
                    },
                    'code_example': '''
import umap
reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2)
embedding = reducer.fit_transform(neuron_activity.T)
                    ''',
                    'interpretation': 'åœ¨ä½ç»´ç©ºé—´å¯è§†åŒ–ç¥ç»å…ƒæ´»åŠ¨æ¨¡å¼'
                }
            ]
        })
        
        # 3. å› æœåˆ†ææ¨¡å‹
        models.append({
            'category': 'å› æœå…³ç³»åˆ†æ',
            'models': [
                {
                    'name': 'Grangerå› æœæ£€éªŒ',
                    'parameters': {
                        'maxlag': '1-5 (æœ€å¤§æ»åé˜¶æ•°)',
                        'test': 'ssr_ftest, ssr_chi2test',
                        'verbose': 'True/False'
                    },
                    'code_example': '''
from statsmodels.tsa.stattools import grangercausalitytests
# æ£€éªŒç¥ç»å…ƒAæ˜¯å¦Grangerå› æœç¥ç»å…ƒB
result = grangercausalitytests(data[['neuron_B', 'neuron_A']], maxlag=3)
                    ''',
                    'interpretation': 'ç¡®å®šç¥ç»å…ƒé—´çš„å› æœå…³ç³»æ–¹å‘'
                },
                {
                    'name': 'åŠ¨æ€å› æœæ¨¡å‹(DCM)',
                    'parameters': {
                        'TR': 'æ—¶é—´åˆ†è¾¨ç‡',
                        'model_space': 'full, sparse',
                        'estimation': 'VL, ML'
                    },
                    'code_example': '''
# ä½¿ç”¨SPMæˆ–Pythonçš„DCMå®ç°
# å®šä¹‰è¿æ¥çŸ©é˜µå’Œè¾“å…¥çŸ©é˜µ
A = np.zeros((n_regions, n_regions))  # å†…åœ¨è¿æ¥
B = np.zeros((n_regions, n_regions, n_inputs))  # è°ƒåˆ¶è¿æ¥
C = np.zeros((n_regions, n_inputs))  # è¾“å…¥è¿æ¥
                    ''',
                    'interpretation': 'å»ºæ¨¡ç¥ç»ç½‘ç»œçš„æœ‰æ•ˆè¿æ¥'
                }
            ]
        })
        
        # 4. æœºå™¨å­¦ä¹ æ¨¡å‹
        models.append({
            'category': 'é¢„æµ‹æ€§æœºå™¨å­¦ä¹ ',
            'models': [
                {
                    'name': 'LSTMç¥ç»ç½‘ç»œ',
                    'parameters': {
                        'hidden_size': '32-256',
                        'num_layers': '1-3',
                        'dropout': '0.1-0.5',
                        'sequence_length': '5-20'
                    },
                    'code_example': '''
import torch.nn as nn
class NeuralLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])
        return out
                    ''',
                    'interpretation': 'é¢„æµ‹æœªæ¥çš„è¡Œä¸ºçŠ¶æ€æˆ–ç¥ç»æ´»åŠ¨'
                }
            ]
        })
        
        # æ‰“å°æ¨¡å‹å»ºè®®
        for category_info in models:
            print(f"\nğŸ“Š {category_info['category']}")
            for model in category_info['models']:
                print(f"\n  ğŸ”§ {model['name']}")
                print("     å‚æ•°å»ºè®®:")
                for param, value in model['parameters'].items():
                    print(f"       {param}: {value}")
                print(f"     åº”ç”¨: {model['interpretation']}")
                print("     ä»£ç ç¤ºä¾‹:")
                print(model['code_example'])
        
        return models
    
    def generate_analysis_workflow(self, characteristics):
        """ç”Ÿæˆå®Œæ•´çš„åˆ†æå·¥ä½œæµç¨‹"""
        print("\n=== å»ºè®®çš„åˆ†æå·¥ä½œæµç¨‹ ===")
        
        workflow = [
            {
                'step': 1,
                'name': 'æ•°æ®é¢„å¤„ç†ä¸è´¨é‡æ§åˆ¶',
                'tasks': [
                    'æ£€æŸ¥æ•°æ®å®Œæ•´æ€§å’Œå¼‚å¸¸å€¼',
                    'æ ‡å‡†åŒ–æˆ–å½’ä¸€åŒ–ç¥ç»å…ƒæ´»åŠ¨æ•°æ®',
                    'å»é™¤è¶‹åŠ¿å’Œå­£èŠ‚æ€§æˆåˆ†ï¼ˆå¦‚é€‚ç”¨ï¼‰',
                    'æ•°æ®å¹³æ»‘ï¼ˆç§»åŠ¨å¹³å‡æˆ–é«˜æ–¯æ»¤æ³¢ï¼‰'
                ],
                'tools': ['pandas', 'scipy.signal', 'sklearn.preprocessing'],
                'duration': '1-2å¤©'
            },
            {
                'step': 2,
                'name': 'æ¢ç´¢æ€§æ•°æ®åˆ†æ',
                'tasks': [
                    'å¯è§†åŒ–ç¥ç»å…ƒæ´»åŠ¨æ—¶é—´åºåˆ—',
                    'åˆ†æè¡Œä¸ºè½¬æ¢æ¨¡å¼',
                    'è®¡ç®—ç¥ç»å…ƒé—´ç›¸å…³æ€§çŸ©é˜µ',
                    'è¯†åˆ«æ´»åŠ¨å³°å€¼å’Œå¼‚å¸¸æ¨¡å¼'
                ],
                'tools': ['matplotlib', 'seaborn', 'plotly'],
                'duration': '2-3å¤©'
            },
            {
                'step': 3,
                'name': 'æ¨¡å¼è¯†åˆ«ä¸èšç±»',
                'tasks': [
                    'æ‰§è¡Œå±‚æ¬¡èšç±»åˆ†æ',
                    'è¿›è¡Œä¸»æˆåˆ†åˆ†æ(PCA)',
                    'ä½¿ç”¨UMAPè¿›è¡Œéçº¿æ€§é™ç»´',
                    'éªŒè¯èšç±»ç»“æœçš„ç¨³å®šæ€§'
                ],
                'tools': ['scikit-learn', 'umap-learn', 'scipy.cluster'],
                'duration': '3-4å¤©'
            },
            {
                'step': 4,
                'name': 'æ—¶é—´åŠ¨åŠ›å­¦åˆ†æ',
                'tasks': [
                    'æ‹ŸåˆARIMAæ—¶é—´åºåˆ—æ¨¡å‹',
                    'è¿›è¡Œéšé©¬å°”å¯å¤«æ¨¡å‹åˆ†æ',
                    'è®¡ç®—ç¥ç»å…ƒæ´»åŠ¨çš„è‡ªç›¸å…³å‡½æ•°',
                    'åˆ†æå‘¨æœŸæ€§å’Œè¶‹åŠ¿æˆåˆ†'
                ],
                'tools': ['statsmodels', 'hmmlearn', 'pykalman'],
                'duration': '4-5å¤©'
            },
            {
                'step': 5,
                'name': 'å› æœå…³ç³»åˆ†æ',
                'tasks': [
                    'Grangerå› æœæ£€éªŒ',
                    'æ„å»ºæœ‰å‘å›¾ç½‘ç»œ',
                    'åˆ†æä¿¡æ¯æµæ–¹å‘',
                    'éªŒè¯å› æœå…³ç³»çš„é²æ£’æ€§'
                ],
                'tools': ['statsmodels', 'networkx', 'causalgraphicalmodels'],
                'duration': '3-4å¤©'
            },
            {
                'step': 6,
                'name': 'é¢„æµ‹å»ºæ¨¡',
                'tasks': [
                    'ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©',
                    'è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹',
                    'äº¤å‰éªŒè¯å’Œè¶…å‚æ•°è°ƒä¼˜',
                    'æ¨¡å‹è§£é‡Šå’Œé‡è¦æ€§åˆ†æ'
                ],
                'tools': ['scikit-learn', 'tensorflow/pytorch', 'shap'],
                'duration': '5-7å¤©'
            },
            {
                'step': 7,
                'name': 'ç»“æœéªŒè¯ä¸è§£é‡Š',
                'tasks': [
                    'ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ',
                    'æ•ˆåº”é‡è®¡ç®—',
                    'æ•æ„Ÿæ€§åˆ†æ',
                    'ç”Ÿç‰©å­¦æ„ä¹‰è§£é‡Š'
                ],
                'tools': ['scipy.stats', 'statsmodels', 'pingouin'],
                'duration': '3-4å¤©'
            },
            {
                'step': 8,
                'name': 'å¯è§†åŒ–ä¸æŠ¥å‘Š',
                'tasks': [
                    'åˆ›å»ºç»¼åˆåˆ†æå›¾è¡¨',
                    'åˆ¶ä½œäº¤äº’å¼å¯è§†åŒ–',
                    'æ’°å†™åˆ†ææŠ¥å‘Š',
                    'å‡†å¤‡å­¦æœ¯è®ºæ–‡å›¾è¡¨'
                ],
                'tools': ['matplotlib', 'plotly', 'seaborn', 'bokeh'],
                'duration': '3-5å¤©'
            }
        ]
        
        print("ğŸ“‹ å®Œæ•´åˆ†æå·¥ä½œæµç¨‹:")
        total_duration = 0
        
        for step_info in workflow:
            print(f"\næ­¥éª¤ {step_info['step']}: {step_info['name']} ({step_info['duration']})")
            print("    ä»»åŠ¡:")
            for task in step_info['tasks']:
                print(f"      â€¢ {task}")
            print(f"    æ¨èå·¥å…·: {', '.join(step_info['tools'])}")
            
            # æå–æŒç»­æ—¶é—´çš„æ•°å­—éƒ¨åˆ†æ¥è®¡ç®—æ€»æ—¶é—´
            duration_parts = step_info['duration'].split('-')
            if len(duration_parts) == 2:
                avg_duration = (int(duration_parts[0]) + int(duration_parts[1].split('å¤©')[0])) / 2
            else:
                avg_duration = int(duration_parts[0].split('å¤©')[0])
            total_duration += avg_duration
        
        print(f"\nâ±ï¸  é¢„è®¡æ€»æ—¶é—´: {total_duration:.0f}å¤© ({total_duration/7:.1f}å‘¨)")
        
        return workflow
    
    def create_research_proposal_template(self):
        """ç”Ÿæˆç ”ç©¶ææ¡ˆæ¨¡æ¿"""
        print("\n=== ç ”ç©¶ææ¡ˆæ¨¡æ¿ ===")
        
        template = f"""
# å°é¼ ç¥ç»å…ƒæ´»åŠ¨æ—¶é—´æ¨¡å¼ç ”ç©¶ææ¡ˆ

## 1. ç ”ç©¶èƒŒæ™¯ä¸ç›®æ ‡
**ç ”ç©¶é—®é¢˜**: å°é¼ åœ¨è¡Œä¸ºè½¬æ¢è¿‡ç¨‹ä¸­ç¥ç»å…ƒæ´»åŠ¨çš„æ—¶é—´åŠ¨æ€æ¨¡å¼
**å…·ä½“ç›®æ ‡**:
- è¯†åˆ«è¡Œä¸ºè½¬æ¢ç›¸å…³çš„ç¥ç»æ´»åŠ¨æ¨¡å¼
- å»ºç«‹ç¥ç»æ´»åŠ¨ä¸è¡Œä¸ºçŠ¶æ€çš„é¢„æµ‹æ¨¡å‹
- æ­ç¤ºç¥ç»å…ƒç½‘ç»œçš„åŠŸèƒ½ç»„ç»‡åŸç†

## 2. æ•°æ®ç‰¹å¾
- æ—¶é—´ç‚¹æ•°é‡: {self.clustering_result.get('activity_matrix', pd.DataFrame()).shape[1] if self.clustering_result else 'N/A'}
- ç¥ç»å…ƒæ•°é‡: {len(self.clustering_result.get('clusters', {}).get(1, [])) if self.clustering_result else 'N/A'}
- è¡Œä¸ºç±»å‹: {len(self.response_patterns) if self.response_patterns else 'N/A'}

## 3. æ–¹æ³•å­¦æ¡†æ¶
### 3.1 æ—¶é—´åºåˆ—åˆ†æ
- **ARIMAæ¨¡å‹**: åˆ†æç¥ç»å…ƒæ´»åŠ¨çš„è‡ªå›å½’ç‰¹æ€§
- **éšé©¬å°”å¯å¤«æ¨¡å‹**: è¯†åˆ«æ½œåœ¨çš„ç¥ç»çŠ¶æ€
- **çŠ¶æ€ç©ºé—´æ¨¡å‹**: è·Ÿè¸ªåŠ¨æ€å˜åŒ–è¿‡ç¨‹

### 3.2 æ¨¡å¼è¯†åˆ«
- **èšç±»åˆ†æ**: è¯†åˆ«åŠŸèƒ½ç›¸ä¼¼çš„ç¥ç»å…ƒç¾¤
- **ä¸»æˆåˆ†åˆ†æ**: é™ç»´å’Œæ¨¡å¼æå–
- **è°±èšç±»**: åŸºäºç›¸ä¼¼æ€§çš„ç½‘ç»œåˆ’åˆ†

### 3.3 å› æœæ¨æ–­
- **Grangerå› æœæ£€éªŒ**: ç¡®å®šç¥ç»å…ƒé—´çš„å› æœå…³ç³»
- **åŠ¨æ€å› æœæ¨¡å‹**: å»ºæ¨¡æœ‰æ•ˆè¿æ¥
- **ä¿¡æ¯è®ºæ–¹æ³•**: é‡åŒ–ä¿¡æ¯ä¼ é€’

### 3.4 é¢„æµ‹å»ºæ¨¡
- **æœºå™¨å­¦ä¹ **: æ”¯æŒå‘é‡æœºã€éšæœºæ£®æ—
- **æ·±åº¦å­¦ä¹ **: LSTMã€GRUæ—¶é—´åºåˆ—æ¨¡å‹
- **é›†æˆæ–¹æ³•**: æé«˜é¢„æµ‹å‡†ç¡®æ€§

## 4. é¢„æœŸæˆæœ
1. **ç§‘å­¦å‘ç°**:
   - ç¥ç»å…ƒæ´»åŠ¨çš„æ—¶é—´åŠ¨åŠ›å­¦è§„å¾‹
   - è¡Œä¸ºè½¬æ¢çš„ç¥ç»æœºåˆ¶
   - ç¥ç»ç½‘ç»œçš„åŠŸèƒ½æ¨¡å—

2. **æ–¹æ³•å­¦è´¡çŒ®**:
   - å¤šæ¨¡æ€æ—¶é—´åºåˆ—åˆ†ææ¡†æ¶
   - ç¥ç»æ´»åŠ¨é¢„æµ‹æ¨¡å‹
   - å¯é‡å¤çš„åˆ†æç®¡é“

3. **åº”ç”¨ä»·å€¼**:
   - ç¥ç»ç–¾ç—…çš„æ—©æœŸè¯Šæ–­
   - è„‘æœºæ¥å£æŠ€æœ¯
   - è¯ç‰©ç­›é€‰å¹³å°

## 5. æ—¶é—´å®‰æ’
- **ç¬¬1-2å‘¨**: æ•°æ®é¢„å¤„ç†ä¸è´¨é‡æ§åˆ¶
- **ç¬¬3-4å‘¨**: æ¢ç´¢æ€§æ•°æ®åˆ†æ
- **ç¬¬5-6å‘¨**: æ¨¡å¼è¯†åˆ«ä¸èšç±»
- **ç¬¬7-8å‘¨**: æ—¶é—´åŠ¨åŠ›å­¦åˆ†æ
- **ç¬¬9-10å‘¨**: å› æœå…³ç³»åˆ†æ
- **ç¬¬11-12å‘¨**: é¢„æµ‹å»ºæ¨¡
- **ç¬¬13-14å‘¨**: ç»“æœéªŒè¯ä¸è§£é‡Š
- **ç¬¬15-16å‘¨**: å¯è§†åŒ–ä¸æŠ¥å‘Š

## 6. é£é™©ä¸å¯¹ç­–
**æ½œåœ¨é£é™©**:
- æ•°æ®è´¨é‡é—®é¢˜ â†’ ä¸¥æ ¼çš„è´¨æ§æµç¨‹
- æ¨¡å‹è¿‡æ‹Ÿåˆ â†’ äº¤å‰éªŒè¯å’Œæ­£åˆ™åŒ–
- è®¡ç®—èµ„æºé™åˆ¶ â†’ äº‘è®¡ç®—å¹³å°

**è´¨é‡ä¿è¯**:
- å¯é‡å¤æ€§æ£€éªŒ
- å¤šç§æ–¹æ³•éªŒè¯
- ä¸“å®¶åŒè¡Œè¯„è®®

## 7. èµ„æºéœ€æ±‚
**è½¯ä»¶å·¥å…·**: Pythonç§‘å­¦è®¡ç®—æ ˆ (pandas, scikit-learn, tensorflow)
**ç¡¬ä»¶éœ€æ±‚**: GPUåŠ é€Ÿè®¡ç®— (æ·±åº¦å­¦ä¹ æ¨¡å‹)
**äººåŠ›èµ„æº**: æ•°æ®ç§‘å­¦å®¶ + ç¥ç»ç§‘å­¦ä¸“å®¶
**æ—¶é—´æŠ•å…¥**: 4ä¸ªæœˆå…¨èŒå·¥ä½œé‡

## 8. ä¼¦ç†è€ƒè™‘
- åŠ¨ç‰©å®éªŒä¼¦ç†å®¡æŸ¥
- æ•°æ®ä½¿ç”¨æˆæƒ
- ç»“æœå‘å¸ƒè§„èŒƒ
        """
        
        print(template)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open('research_proposal_template.md', 'w', encoding='utf-8') as f:
            f.write(template)
        
        print("\nğŸ“„ ç ”ç©¶ææ¡ˆæ¨¡æ¿å·²ä¿å­˜åˆ° 'research_proposal_template.md'")
        
        return template

def create_methodology_report(data_path):
    """åˆ›å»ºå®Œæ•´çš„æ–¹æ³•å­¦å»ºè®®æŠ¥å‘Š"""
    print("æ­£åœ¨ç”Ÿæˆç ”ç©¶æ–¹æ³•å­¦å»ºè®®æŠ¥å‘Š...")
    
    # åŠ è½½æ•°æ®è¿›è¡Œåˆ†æ
    from temporal_pattern_analysis import TemporalPatternAnalyzer
    
    analyzer = TemporalPatternAnalyzer(data_path)
    df = analyzer.load_and_preprocess_data()
    
    # è¿è¡Œå„ç§åˆ†æ
    temporal_patterns = analyzer.analyze_sequential_patterns()
    clustering_result = analyzer.cluster_neuron_patterns()
    response_patterns = analyzer.identify_response_patterns()
    
    # åˆ›å»ºæ–¹æ³•å­¦å»ºè®®å™¨
    advisor = ResearchMethodologyAdvisor(temporal_patterns, clustering_result, response_patterns)
    
    # åˆ†ææ•°æ®ç‰¹å¾
    characteristics = advisor.analyze_data_characteristics(df)
    
    # ç”Ÿæˆå„ç§å»ºè®®
    paradigms = advisor.suggest_research_paradigm(characteristics)
    models = advisor.suggest_specific_models(characteristics)
    workflow = advisor.generate_analysis_workflow(characteristics)
    proposal = advisor.create_research_proposal_template()
    
    return {
        'characteristics': characteristics,
        'paradigms': paradigms,
        'models': models,
        'workflow': workflow,
        'proposal': proposal
    }

if __name__ == "__main__":
    data_path = 'data/EMtrace01-å¤šæ ‡ç­¾ç‰ˆ.csv'
    report = create_methodology_report(data_path) 